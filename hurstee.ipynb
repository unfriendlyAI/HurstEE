{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de67c4d",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-12-29T18:07:00.296051Z",
     "iopub.status.busy": "2025-12-29T18:07:00.295564Z",
     "iopub.status.idle": "2025-12-29T18:08:46.223925Z",
     "shell.execute_reply": "2025-12-29T18:08:46.222464Z"
    },
    "papermill": {
     "duration": 105.945942,
     "end_time": "2025-12-29T18:08:46.225920",
     "exception": false,
     "start_time": "2025-12-29T18:07:00.279978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q mfdfa\n",
    "!pip install -q andi-datasets \n",
    "!pip install -q stochastic\n",
    "!pip install -q tensorflow==2.8.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62cc91",
   "metadata": {
    "papermill": {
     "duration": 0.022778,
     "end_time": "2025-12-29T18:08:46.272135",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.249357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HurstEE: Differentiable Neural Network Layer for Estimating Hurst and Anomalous Diffusion Exponents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22416488",
   "metadata": {
    "papermill": {
     "duration": 0.02284,
     "end_time": "2025-12-29T18:08:46.318649",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.295809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We introduce an implementation of the Time-Averaged Mean Squared Displacement (TA-MSD) method, equivalent to the 2nd-order Generalized Hurst Exponent (GHE) method, as a non-trainable, differentiable layer in TensorFlow and PyTorch frameworks. Our proposed layer effectively addresses the core challenges identified in the 2nd AnDi Challenge, including trajectory heterogeneity and short trajectory segments. The layer also seamlessly handles missing (NaN) values, facilitating integration into complex neural network architectures designed for analyzing heterogeneous trajectories. The proposed differentiable Hurst exponent estimation layer offers simplicity in deployment, eliminating the need for training and reducing computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6244ff7",
   "metadata": {
    "papermill": {
     "duration": 0.02265,
     "end_time": "2025-12-29T18:08:46.364214",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.341564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparison of HurstEE vs various Hurst exponent estimation methods on AnDi-2020 and FBM Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26de4448",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:46.411190Z",
     "iopub.status.busy": "2025-12-29T18:08:46.410829Z",
     "iopub.status.idle": "2025-12-29T18:08:46.415584Z",
     "shell.execute_reply": "2025-12-29T18:08:46.414508Z"
    },
    "papermill": {
     "duration": 0.030388,
     "end_time": "2025-12-29T18:08:46.417337",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.386949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/randimodel/randi-main')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d60e47",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:46.464945Z",
     "iopub.status.busy": "2025-12-29T18:08:46.464568Z",
     "iopub.status.idle": "2025-12-29T18:08:46.468838Z",
     "shell.execute_reply": "2025-12-29T18:08:46.467854Z"
    },
    "papermill": {
     "duration": 0.029939,
     "end_time": "2025-12-29T18:08:46.470494",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.440555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d76bf",
   "metadata": {
    "papermill": {
     "duration": 0.022508,
     "end_time": "2025-12-29T18:08:46.515916",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.493408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HurstEE with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5acf17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:46.563523Z",
     "iopub.status.busy": "2025-12-29T18:08:46.563136Z",
     "iopub.status.idle": "2025-12-29T18:08:55.622008Z",
     "shell.execute_reply": "2025-12-29T18:08:55.620965Z"
    },
    "papermill": {
     "duration": 9.084707,
     "end_time": "2025-12-29T18:08:55.623902",
     "exception": false,
     "start_time": "2025-12-29T18:08:46.539195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class HurstEE(tf.keras.layers.Layer):\n",
    "    def __init__(self, use_correction=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialization of the HurstEE layer.\n",
    "\n",
    "        Args:\n",
    "            use_correction (bool): Whether to apply TEA-MSD&variance correction. Default is False.\n",
    "\n",
    "        \"\"\"\n",
    "        super(HurstEE, self).__init__(**kwargs)\n",
    "        self.use_correction = use_correction\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def nanmean(tensor, axis=None, keepdims=False):\n",
    "        \"\"\"\n",
    "        Analog of np.nanmean, ignoring NaN values.\n",
    "\n",
    "        Args:\n",
    "            tensor (tf.Tensor): Input tensor.\n",
    "            axis (int or tuple, optional): Axis or axes along which to compute the mean.\n",
    "            keepdims (bool, optional): If True, retains reduced dimensions.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Mean value with NaNs ignored.\n",
    "        \"\"\"\n",
    "        mask = ~tf.math.is_nan(tensor)\n",
    "        masked_tensor = tf.where(mask, tensor, tf.zeros_like(tensor))\n",
    "        count = tf.reduce_sum(tf.cast(mask, tf.float32), axis=axis, keepdims=keepdims)\n",
    "        sum_ = tf.reduce_sum(masked_tensor, axis=axis, keepdims=keepdims)\n",
    "        return sum_ / (count + 1e-8)\n",
    "\n",
    "    @staticmethod\n",
    "    def nanstd(tensor, axis=None, keepdims=False):\n",
    "        \"\"\"\n",
    "        Analog of np.nanstd, ignoring NaN values.\n",
    "\n",
    "        Args:\n",
    "            tensor (tf.Tensor): Input tensor.\n",
    "            axis (int or tuple, optional): Axis or axes along which to compute the standard deviation.\n",
    "            keepdims (bool, optional): If True, retains reduced dimensions.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Standard deviation with NaNs ignored.\n",
    "        \"\"\"\n",
    "        mean_ = HurstEE.nanmean(tensor, axis=axis, keepdims=True)\n",
    "        sq_diff = tf.square(tensor - mean_)\n",
    "        var_ = HurstEE.nanmean(sq_diff, axis=axis, keepdims=keepdims)\n",
    "        return tf.sqrt(var_ + 1e-8)\n",
    "\n",
    "\n",
    "    def tf_normalize_distribution(self, arr, target_mean, T):\n",
    "        \"\"\"\n",
    "        Normalize the distribution with correction.\n",
    "\n",
    "        Args:\n",
    "            arr (tf.Tensor): Input array.\n",
    "            target_mean (tf.Tensor): Target mean value.\n",
    "            T (tf.Tensor): Effective time/count indicator.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Normalized array.\n",
    "        \"\"\"\n",
    "        current_mean = self.nanmean(arr)\n",
    "        current_std = self.nanstd(arr)\n",
    "        adjustment_term = tf.maximum(current_std**2 - 0.92 / T, 0)**0.5\n",
    "        standardized = (arr - current_mean) / (current_std + 1e-8)\n",
    "        return standardized * adjustment_term + target_mean\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape (batch_size, timesteps, input_channels).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Estimated alpha values.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        timesteps = tf.shape(inputs)[1]\n",
    "        input_channels = tf.shape(inputs)[2]\n",
    "\n",
    "        # Count non-NaN elements across the batch\n",
    "        not_nan_mask = ~tf.math.is_nan(inputs)\n",
    "        not_nan_count = tf.reduce_sum(tf.cast(not_nan_mask, tf.float32))\n",
    "\n",
    "        epsilon = 1e-14\n",
    "        x_all = inputs\n",
    "        max_lag = 4\n",
    "        msds = []\n",
    "        teamsd_list = []\n",
    "\n",
    "        # --- Compute MSD and TEA-MSD ---\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            displacements = x_all[:, lag:, :] - x_all[:, :-lag, :]\n",
    "            squared_displacements = tf.square(displacements)\n",
    "            squared_displacements_sum = tf.reduce_sum(squared_displacements, axis=2)\n",
    "\n",
    "            # For MSD – trim each row\n",
    "            msd = self.nanmean(\n",
    "                squared_displacements_sum,\n",
    "                axis=1\n",
    "            )\n",
    "            msds.append(msd)\n",
    "\n",
    "            # For TEA-MSD – trim the array as a whole\n",
    "            teamsd_lag = self.nanmean(\n",
    "                squared_displacements_sum\n",
    "            )\n",
    "            teamsd_list.append(teamsd_lag)\n",
    "\n",
    "        msds = tf.stack(msds, axis=1)             # (batch_size, max_lag)\n",
    "        teamsd = tf.stack(teamsd_list, axis=0)    # (max_lag,)\n",
    "\n",
    "        t_lags = tf.range(1, max_lag + 1, dtype=tf.float32)\n",
    "        log_t_lags = tf.math.log(t_lags + epsilon)\n",
    "\n",
    "        # --- Compute TEA-MSD alpha ---\n",
    "        log_teamsd = tf.math.log(teamsd + epsilon)\n",
    "        mean_x = self.nanmean(log_t_lags)\n",
    "        mean_x2 = self.nanmean(tf.square(log_t_lags))\n",
    "        mean_y_teamsd = self.nanmean(log_teamsd)\n",
    "        mean_xy_teamsd = self.nanmean(log_t_lags * log_teamsd)\n",
    "\n",
    "        numerator_teamsd = mean_xy_teamsd - mean_x * mean_y_teamsd\n",
    "        denominator = mean_x2 - mean_x**2\n",
    "        alpha_teamsd = numerator_teamsd / denominator\n",
    "\n",
    "        # --- Compute alpha for the batch ---\n",
    "        log_msds = tf.math.log(msds + epsilon)   # (batch_size, max_lag)\n",
    "        mean_y = self.nanmean(log_msds, axis=1)  # (batch_size,)\n",
    "        mean_xy = self.nanmean(log_t_lags * log_msds, axis=1)\n",
    "        numerator = mean_xy - mean_x * mean_y\n",
    "        alpha = numerator / denominator          # (batch_size,)\n",
    "\n",
    "        # --- Replace alpha with NaN if the last MSD < 1e-6 ---\n",
    "        last_msd = msds[:, -1]  # (batch_size,)\n",
    "        mask_nan = last_msd < 1e-6\n",
    "        alpha = tf.where(mask_nan, tf.fill(tf.shape(alpha), np.nan), alpha)\n",
    "\n",
    "        # --- Compute T based on the number of valid trajectories ---\n",
    "        valid_mask = ~mask_nan\n",
    "        valid_count = tf.reduce_sum(tf.cast(valid_mask, tf.float32))\n",
    "        T = not_nan_count / (valid_count * tf.cast(input_channels, tf.float32))\n",
    "\n",
    "        # --- Apply normalization correction if enabled ---\n",
    "        if self.use_correction:\n",
    "            alpha = self.tf_normalize_distribution(alpha, alpha_teamsd, T)\n",
    "\n",
    "        # --- Clip alpha to the valid range ---\n",
    "        H = tf.clip_by_value(alpha, clip_value_min=0.0001, clip_value_max=1.999) / 2\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b5c46",
   "metadata": {
    "papermill": {
     "duration": 0.022643,
     "end_time": "2025-12-29T18:08:55.669902",
     "exception": false,
     "start_time": "2025-12-29T18:08:55.647259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HurstEE with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86282164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:55.718145Z",
     "iopub.status.busy": "2025-12-29T18:08:55.717497Z",
     "iopub.status.idle": "2025-12-29T18:09:01.207723Z",
     "shell.execute_reply": "2025-12-29T18:09:01.206479Z"
    },
    "papermill": {
     "duration": 5.517332,
     "end_time": "2025-12-29T18:09:01.209783",
     "exception": false,
     "start_time": "2025-12-29T18:08:55.692451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PyHurstEE(nn.Module):\n",
    "    def __init__(self, use_correction=False):\n",
    "        super(PyHurstEE, self).__init__()\n",
    "        self.use_correction = use_correction\n",
    "\n",
    "    @staticmethod\n",
    "    def nanmean(tensor, dim=None, keepdim=False, eps=1e-8):\n",
    "        mask = ~torch.isnan(tensor)\n",
    "        masked = torch.where(mask, tensor, torch.zeros_like(tensor))\n",
    "        count = mask.sum(dim=dim, keepdim=keepdim).to(tensor.dtype)\n",
    "        sum_ = masked.sum(dim=dim, keepdim=keepdim)\n",
    "        return sum_ / (count + eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def nanstd(tensor, dim=None, keepdim=False, eps=1e-8):\n",
    "        mean_ = PyHurstEE.nanmean(tensor, dim=dim, keepdim=True, eps=eps)\n",
    "        sq_diff = (tensor - mean_)**2\n",
    "        var = PyHurstEE.nanmean(sq_diff, dim=dim, keepdim=keepdim, eps=eps)\n",
    "        return torch.sqrt(var + eps)\n",
    "\n",
    "\n",
    "    def normalize_distribution(self, arr, target_mean, T, eps=1e-8):\n",
    "        current_mean = self.nanmean(arr)\n",
    "        current_std = self.nanstd(arr)\n",
    "        adjustment = torch.sqrt(torch.clamp(current_std**2 - 0.92 / T, min=0.0))\n",
    "        standardized = (arr - current_mean) / (current_std + eps)\n",
    "        return standardized * adjustment + target_mean\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).float()\n",
    "        batch, timesteps, channels = inputs.shape\n",
    "        not_nan = ~torch.isnan(inputs)\n",
    "        not_nan_count = not_nan.sum().to(inputs.dtype)\n",
    "\n",
    "        max_lag = 4\n",
    "        msds = []\n",
    "        teamsd_list = []\n",
    "\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            disp = inputs[:, lag:, :] - inputs[:, :-lag, :]\n",
    "            sq = disp**2\n",
    "            sq_sum = sq.sum(dim=2)\n",
    "\n",
    "            msd = PyHurstEE.nanmean(sq_sum, dim=1)\n",
    "            msds.append(msd)\n",
    "\n",
    "\n",
    "            teamsd_lag = PyHurstEE.nanmean(sq_sum)\n",
    "            teamsd_list.append(teamsd_lag)\n",
    "\n",
    "        msds = torch.stack(msds, dim=1)\n",
    "        teamsd = torch.stack(teamsd_list)\n",
    "\n",
    "        eps = 1e-14\n",
    "        t_lags = torch.arange(1, max_lag+1, dtype=inputs.dtype, device=inputs.device)\n",
    "        log_t = torch.log(t_lags + eps)\n",
    "\n",
    "        log_teamsd = torch.log(teamsd + eps)\n",
    "        mean_x = PyHurstEE.nanmean(log_t)\n",
    "        mean_x2 = PyHurstEE.nanmean(log_t**2)\n",
    "        mean_y_tea = PyHurstEE.nanmean(log_teamsd)\n",
    "        mean_xy_tea = PyHurstEE.nanmean(log_t * log_teamsd)\n",
    "        num_tea = mean_xy_tea - mean_x * mean_y_tea\n",
    "        den = mean_x2 - mean_x**2\n",
    "        alpha_tea = num_tea / den\n",
    "\n",
    "        log_msds = torch.log(msds + eps)\n",
    "        mean_y = PyHurstEE.nanmean(log_msds, dim=1)\n",
    "        mean_xy = PyHurstEE.nanmean(log_t * log_msds, dim=1)\n",
    "        num = mean_xy - mean_x * mean_y\n",
    "        alpha = num / den\n",
    "\n",
    "        last = msds[:, -1]\n",
    "        mask = last < 1e-6\n",
    "        alpha = alpha.masked_fill(mask, float('nan'))\n",
    "\n",
    "        valid = ~mask\n",
    "        valid_count = valid.sum().to(inputs.dtype)\n",
    "        T = not_nan_count / (valid_count * channels)\n",
    "\n",
    "        if self.use_correction:\n",
    "            alpha = self.normalize_distribution(alpha, alpha_tea, T)\n",
    "\n",
    "        H = torch.clamp(alpha, min=0.0001, max=1.999) / 2\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a084447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:01.258452Z",
     "iopub.status.busy": "2025-12-29T18:09:01.258061Z",
     "iopub.status.idle": "2025-12-29T18:09:03.144189Z",
     "shell.execute_reply": "2025-12-29T18:09:03.143125Z"
    },
    "papermill": {
     "duration": 1.912158,
     "end_time": "2025-12-29T18:09:03.146100",
     "exception": false,
     "start_time": "2025-12-29T18:09:01.233942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import andi_datasets\n",
    "from andi_datasets.datasets_challenge import challenge_theory_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils import data_norm, data_reshape, many_net_uhd, my_atan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613d10a",
   "metadata": {
    "papermill": {
     "duration": 0.022209,
     "end_time": "2025-12-29T18:09:03.191554",
     "exception": false,
     "start_time": "2025-12-29T18:09:03.169345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RANDI weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25fcf82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:03.238529Z",
     "iopub.status.busy": "2025-12-29T18:09:03.237856Z",
     "iopub.status.idle": "2025-12-29T18:09:04.809098Z",
     "shell.execute_reply": "2025-12-29T18:09:04.808004Z"
    },
    "papermill": {
     "duration": 1.596892,
     "end_time": "2025-12-29T18:09:04.811035",
     "exception": false,
     "start_time": "2025-12-29T18:09:03.214143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "randi25 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_25.h5')\n",
    "randi65 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_65.h5')  \n",
    "randi125 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_125.h5')\n",
    "randi225 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_225.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d6eb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:04.858851Z",
     "iopub.status.busy": "2025-12-29T18:09:04.858490Z",
     "iopub.status.idle": "2025-12-29T18:09:05.184791Z",
     "shell.execute_reply": "2025-12-29T18:09:05.183826Z"
    },
    "papermill": {
     "duration": 0.352747,
     "end_time": "2025-12-29T18:09:05.186577",
     "exception": false,
     "start_time": "2025-12-29T18:09:04.833830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculate_Hw(X, wavelet=\"db4\"):\n",
    "    \"\"\"\n",
    "    Xcum -  time series\n",
    "    wavelet - name of wavelet\n",
    "    level - decomposition level\n",
    "    \"\"\"\n",
    "    \n",
    "    level = int(np.floor(np.log2(len(X))) - 1)\n",
    "    # Wavelet transformation\n",
    "    coeffs = pywt.wavedec(X, wavelet, level=level, mode=\"periodization\") # mode='symmetric'\n",
    "    \n",
    "    # Wavelet energy\n",
    "    E = np.array(list(reversed([np.sum(i ** 2)/len(i) for i in coeffs[1:]])))\n",
    "  \n",
    "    # Calculation of Hurst exponent\n",
    "    reg = LinearRegression().fit(np.arange(len(coeffs) - 1).reshape(-1, 1), np.log2(E))\n",
    "    H = np.abs((reg.coef_ + 1) / 2)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9df4199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:05.234126Z",
     "iopub.status.busy": "2025-12-29T18:09:05.233240Z",
     "iopub.status.idle": "2025-12-29T18:09:05.241673Z",
     "shell.execute_reply": "2025-12-29T18:09:05.240698Z"
    },
    "papermill": {
     "duration": 0.033797,
     "end_time": "2025-12-29T18:09:05.243479",
     "exception": false,
     "start_time": "2025-12-29T18:09:05.209682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MFDFA import MFDFA\n",
    "def mf_dfa(y):\n",
    "\n",
    "    lag = np.arange(1,24)\n",
    "\n",
    "    q_list = [1,2]  \n",
    "    \n",
    "    lag, Fq = MFDFA(y, lag=lag, q=q_list, order=1)  # Fq.shape = (len(lag), len(q))\n",
    "    \n",
    "    log_lag = np.log(lag)\n",
    "    h_q = np.array([\n",
    "        np.polyfit(log_lag, np.log(Fq[:, i]), 1)[0]\n",
    "        for i in range(len(q_list))\n",
    "    ])\n",
    "\n",
    "  \n",
    "    return np.clip(h_q[1], 0.001, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff80a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:05.291477Z",
     "iopub.status.busy": "2025-12-29T18:09:05.291108Z",
     "iopub.status.idle": "2025-12-29T18:09:05.311455Z",
     "shell.execute_reply": "2025-12-29T18:09:05.310285Z"
    },
    "papermill": {
     "duration": 0.045995,
     "end_time": "2025-12-29T18:09:05.313119",
     "exception": false,
     "start_time": "2025-12-29T18:09:05.267124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tamsd:\n",
    "    def __init__(self, use_correction=False, trim_percent=0, is_increments=False, return_TEA=False):\n",
    "        \"\"\"\n",
    "        Ініціалізація класу Tamsd.\n",
    "        Args:\n",
    "            use_correction (bool): Чи застосовувати корекцію TEA-MSD&variance. За замовчуванням False.\n",
    "            trim_percent (int): Відсоток найбільших значень для обрізання. За замовчуванням 0.\n",
    "            is_increments (bool): Чи дані є інкрементами. Якщо False, дані є позиціями (кумулятивними рядами).\n",
    "            return_TEA (bool): Чи повертати TEA exponent замість batch exponents.\n",
    "        \"\"\"\n",
    "        self.use_correction = use_correction\n",
    "        self.trim_percent = trim_percent\n",
    "        self.is_increments = is_increments\n",
    "        self.return_TEA = return_TEA\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_array1s(arr, percent):\n",
    "        \"\"\"\n",
    "        Видаляє заданий відсоток найбільших значень з кожного рядка 2D масиву, ігноруючи NaN.\n",
    "        \n",
    "        Args:\n",
    "            arr (np.ndarray): Вхідний 2D масив.\n",
    "            percent (float): Відсоток значень для видалення (0 <= percent < 100).\n",
    "        Returns:\n",
    "            np.ndarray: Масив зі значеннями вище порогу, заміненими на NaN.\n",
    "        \"\"\"\n",
    "        if percent == 0:\n",
    "            return arr\n",
    "        \n",
    "        result = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            row = arr[i]\n",
    "            valid_mask = ~np.isnan(row)\n",
    "            valid_values = row[valid_mask]\n",
    "            \n",
    "            if len(valid_values) == 0:\n",
    "                continue\n",
    "            \n",
    "            k = int(np.round(len(valid_values) * (percent / 100.0)))\n",
    "            \n",
    "            if k > 0:\n",
    "                # Сортуємо валідні значення і знаходимо поріг\n",
    "                sorted_valid = np.sort(valid_values)\n",
    "                threshold = sorted_valid[-k]\n",
    "                # Замінюємо значення вище порогу на NaN\n",
    "                result[i] = np.where(row < threshold, row, np.nan)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_array1s_tea(arr, percent):\n",
    "        \"\"\"\n",
    "        Видаляє заданий відсоток найбільших значень з усього масиву, ігноруючи NaN.\n",
    "        Кількість елементів для видалення базується на всіх не-NaN значеннях.\n",
    "        Args:\n",
    "            arr (np.ndarray): Вхідний масив.\n",
    "            percent (float): Відсоток значень для видалення (0 <= percent < 100).\n",
    "        Returns:\n",
    "            np.ndarray: Масив зі значеннями вище порогу, заміненими на NaN.\n",
    "        \"\"\"\n",
    "        if percent == 0:\n",
    "            return arr\n",
    "        \n",
    "        valid_mask = ~np.isnan(arr)\n",
    "        valid_values = arr[valid_mask]\n",
    "        \n",
    "        if len(valid_values) == 0:\n",
    "            return arr\n",
    "        \n",
    "        k = int(np.round(len(valid_values) * (percent / 100.0)))\n",
    "        \n",
    "        if k == 0:\n",
    "            return arr\n",
    "        \n",
    "        # Сортуємо валідні значення і знаходимо поріг\n",
    "        sorted_valid = np.sort(valid_values)\n",
    "        threshold = sorted_valid[-k]\n",
    "        # Замінюємо значення вище порогу на NaN\n",
    "        result = np.where(arr < threshold, arr, np.nan)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def normalize_distribution(self, arr, target_mean, T):\n",
    "        \"\"\"\n",
    "        Нормалізація розподілу з корекцією.\n",
    "        Args:\n",
    "            arr (np.ndarray): Вхідний масив.\n",
    "            target_mean (float): Цільове середнє значення.\n",
    "            T (float): Ефективний час/індикатор кількості.\n",
    "        Returns:\n",
    "            np.ndarray: Нормалізований масив.\n",
    "        \"\"\"\n",
    "        current_mean = np.nanmean(arr)\n",
    "        current_std = np.nanstd(arr)\n",
    "        adjustment_term = np.maximum(current_std**2 - 0.92 / T, 0)**0.5\n",
    "        standardized = (arr - current_mean) / (current_std + 1e-8)\n",
    "        return standardized * adjustment_term + target_mean\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Обчислення альфа (показника Херста).\n",
    "        Args:\n",
    "            inputs (np.ndarray): Вхідний масив форми (batch_size, timesteps, input_channels).\n",
    "                                 Якщо is_increments=False, це позиції (кумулятивні ряди).\n",
    "                                 Якщо is_increments=True, це інкременти.\n",
    "        Returns:\n",
    "            np.ndarray: Оцінені значення H (показник Херста).\n",
    "        \"\"\"\n",
    "        # Визначаємо, працюємо з позиціями чи інкрементами\n",
    "        if self.is_increments:\n",
    "            # Якщо дані є інкрементами, перетворюємо їх на позиції (кумулятивні суми)\n",
    "            # Припускаємо, що початкова позиція = 0\n",
    "            positions = np.concatenate([np.zeros((inputs.shape[0], 1, inputs.shape[2])), \n",
    "                                       np.cumsum(inputs, axis=1)], axis=1)\n",
    "        else:\n",
    "            # Дані вже є позиціями\n",
    "            positions = inputs\n",
    "        \n",
    "        batch_size = positions.shape[0]\n",
    "        timesteps = positions.shape[1]\n",
    "        input_channels = positions.shape[2]\n",
    "\n",
    "        # Підрахунок не-NaN елементів\n",
    "        not_nan_mask = ~np.isnan(positions)\n",
    "        not_nan_count = np.sum(not_nan_mask)\n",
    "\n",
    "        epsilon = 0\n",
    "        max_lag = 4\n",
    "        msds = []\n",
    "        teamsd_list = []\n",
    "\n",
    "        # --- Обчислення MSD та TEA-MSD ---\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            # Обчислюємо квадрати зміщень: (position[t+lag] - position[t])^2\n",
    "            # Це еквівалентно підходу в реалізації 2\n",
    "            displacements = positions[:, lag:, :] - positions[:, :-lag, :]\n",
    "            squared_displacements = np.square(displacements)\n",
    "            squared_displacements_sum = np.sum(squared_displacements, axis=2)\n",
    "\n",
    "            # Для MSD – обрізаємо кожен рядок\n",
    "            msd = np.nanmean(\n",
    "                self.trim_array1s(squared_displacements_sum, self.trim_percent),\n",
    "                axis=1\n",
    "            )\n",
    "            msds.append(msd)\n",
    "\n",
    "            # Для TEA-MSD – обрізаємо масив цілком\n",
    "            teamsd_lag = np.nanmean(\n",
    "                self.trim_array1s_tea(squared_displacements_sum, self.trim_percent)\n",
    "            )\n",
    "            teamsd_list.append(teamsd_lag)\n",
    "\n",
    "        msds = np.stack(msds, axis=1)  # (batch_size, max_lag)\n",
    "        teamsd = np.array(teamsd_list)  # (max_lag,)\n",
    "\n",
    "        t_lags = np.arange(1, max_lag + 1, dtype=np.float32)\n",
    "\n",
    "        \n",
    "        if self.return_TEA:\n",
    "\n",
    "            if not np.any(np.isnan(teamsd)):\n",
    "                alpha = np.clip(np.polyfit(np.log(t_lags), np.log(teamsd), deg = 1)[0], 0.001, 1.999)\n",
    "                # print(idx, np.log(row))\n",
    "            else:\n",
    "                alpha = np.NaN\n",
    "                # print('-')\n",
    "            \n",
    "    \n",
    "            # --- Обмеження alpha до валідного діапазону ---\n",
    "            alpha = np.clip(alpha, 0.0001, 1.999) \n",
    "            return alpha   \n",
    "        \n",
    "        alpha = np.zeros(batch_size, dtype= float) \n",
    "        for idx, row in enumerate(msds):\n",
    "            if not np.any(np.isnan(row)):\n",
    "                alpha[idx] = np.polyfit(np.log(t_lags), np.log(row), deg = 1)[0]\n",
    "                # print(idx, np.log(row))\n",
    "            else:\n",
    "                alpha[idx] = np.NaN\n",
    "                # print('-')\n",
    "        \n",
    "        # --- Обмеження alpha до валідного діапазону ---        \n",
    "        alpha = np.clip(alpha, 0.0001, 1.999) \n",
    "        # last_msd = msds[:, -1]  # (batch_size,)\n",
    "        # mask_nan = last_msd < 1e-6\n",
    "        # alpha = np.where(mask_nan, np.nan, alpha)\n",
    "        # alpha = np.where(mask_nan, 1.0, alpha) # використати для уникненн NaN\n",
    "        # alpha = np.nan_to_num(alpha, nan=1.0) # використати для уникненн NaN\n",
    "        \n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ccd341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:05.360759Z",
     "iopub.status.busy": "2025-12-29T18:09:05.360364Z",
     "iopub.status.idle": "2025-12-29T18:09:05.380957Z",
     "shell.execute_reply": "2025-12-29T18:09:05.380012Z"
    },
    "papermill": {
     "duration": 0.046363,
     "end_time": "2025-12-29T18:09:05.382796",
     "exception": false,
     "start_time": "2025-12-29T18:09:05.336433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wes June 21 16:44:17 2023\n",
    "\n",
    "@author: zqfeng\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "\n",
    "\n",
    "class AddMethods(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        None\n",
    "\n",
    "    def Divisors(self, N: int, minimal=20) -> list:\n",
    "        D = []\n",
    "        for i in range(minimal, N // minimal + 1):\n",
    "            if N % i == 0:\n",
    "                D.append(i)\n",
    "        return D\n",
    "\n",
    "    def findOptN(self, N: int, minimal=20) -> int:\n",
    "        \"\"\"\n",
    "        Find such a natural number OptN that possesses the largest number of\n",
    "        divisors among all natural numbers in the interval [0.99*N, N]\n",
    "        \"\"\"\n",
    "        N0 = int(0.99 * N)\n",
    "        # The best length is the one which have more divisiors\n",
    "        Dcount = [len(self.Divisors(i, minimal)) for i in range(N0, N + 1)]\n",
    "        OptN = N0 + Dcount.index(max(Dcount))\n",
    "        return OptN\n",
    "\n",
    "    def OLE_linprog(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Ax = b (Given A & b, try to derive x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : matrix like. With shape m x n.\n",
    "        b : array like. With shape n x 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : Minimal L1 norm solution of the system of equations.\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        YAO Jian-kang. An Algorithm for Minimizing l1-Norm to Overdetermined\n",
    "        Linear Eguations[J]. JIANGXI SCE7NICE, 2007, 25(1): 1-4.\n",
    "        (Available at:\n",
    "        http://d.g.wanfangdata.com.cn/Periodical_jxkx200701002.aspx)\n",
    "\n",
    "        Version: 1.0 writen by z.q.feng @2022.03.13\n",
    "        '''\n",
    "        A, b = np.array(A), np.array(b)\n",
    "        if np.size(A, 0) < np.size(A, 1):\n",
    "            raise ValueError('Matrix A rows must greater than columns!')\n",
    "        m, n = A.shape\n",
    "        # Trans A into two matrix(n x n and (m - n) x n)\n",
    "        A1, A2 = A[:n, :], A[n:, :]\n",
    "        if np.linalg.matrix_rank(A) >= n:\n",
    "            # inverse of A1\n",
    "            A1_ = np.linalg.pinv(A1)\n",
    "        else:\n",
    "            # Generalized inverse of A1\n",
    "            A1_ = np.linalg.pinv(A1)\n",
    "        # c_ij = A2 * A1_\n",
    "        c = np.dot(A2, A1_)\n",
    "        # r(n+1:m) = A2*inv(A1)*r(1:n) + d\n",
    "        d = np.dot(c, b[:n]) - b[n:]\n",
    "        # Basic-Pursuit, target function = sum(u, v)\n",
    "        t = np.ones([2 * m, 1])\n",
    "        # Aeq_ = [c I(m-n)]\n",
    "        Aeq_ = np.hstack([-c, np.eye(m - n, m - n)])\n",
    "        # Aeq[u v] = Aeq_ * (u - v)\n",
    "        Aeq = np.hstack([Aeq_, -Aeq_])\n",
    "        # u, v > 0\n",
    "        bounds = [(0, None) for i in range(2 * m)]\n",
    "        # r0 = [u; v]\n",
    "        r0 = op.linprog(t, A_eq=Aeq, b_eq=d, bounds=bounds,\n",
    "                        method='revised simplex')['x']\n",
    "        # Minimal L1-norm residual vector, r = u - v\n",
    "        r = np.array([r0[:m] - r0[m:]])\n",
    "        # Solving compatible linear equations Ax = b + r\n",
    "        # Generalized inverse solution\n",
    "        x = np.linalg.pinv(A).dot(b + r.T)\n",
    "        return x\n",
    "\n",
    "    def FixedPointSolver(self, fun, x0, eps=1e-6, **kwargs):\n",
    "        \"\"\"\n",
    "        Solving the fixed points.\n",
    "        \"\"\"\n",
    "        k, k_max = 0, 10000\n",
    "        x_guess, dist = x0, 1\n",
    "        while dist > eps and k < k_max:\n",
    "            x_improved = fun(x_guess, **kwargs)\n",
    "            dist = abs(x_improved - x_guess)\n",
    "            x_guess = x_improved\n",
    "            k += 1\n",
    "        return x_guess\n",
    "\n",
    "    def LocalMin(self, fun, interval: list, **kwargs):\n",
    "        \"\"\"\n",
    "            The method used is a combination of  golden  section  search  and\n",
    "        successive parabolic interpolation.  convergence is never much slower\n",
    "        than  that  for  a  Fibonacci search.  If fun has a continuous second\n",
    "        derivative which is positive at the minimum (which is not  at  ax  or\n",
    "        bx),  then  convergence  is  superlinear, and usually of the order of\n",
    "        about  1.324....\n",
    "            The function fun is never evaluated at two points closer together\n",
    "        than  eps*abs(fmin)+(tol/3), where eps is  approximately  the  square\n",
    "        root  of  the  relative  machine  precision.   if  fun  is a unimodal\n",
    "        function and the computed values of  fun  are  always  unimodal  when\n",
    "        separated  by  at least  eps*abs(x)+(tol/3), then  fmin  approximates\n",
    "        the abcissa of the global minimum of fun on the interval  ax,bx  with\n",
    "        an error less than  3*eps*abs(fmin)+tol.  if  fun  is  not  unimodal,\n",
    "        then fmin may approximate a local, but perhaps non-global, minimum to\n",
    "        the same accuracy.\n",
    "            This function subprogram is a slightly modified  version  of  the\n",
    "        python3 procedure  localmin  given in Ref[1] page79.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fun      : Abcissa approximating the point where fun attains a minimum.\n",
    "        interval : Iterative interval of target minimum point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series ts.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Richard Brent, Algorithms for Minimization without Derivatives,\n",
    "            Prentice-Hall, Inc. (1973).\n",
    "\n",
    "        Written by z.q.feng (2023.06.07).\n",
    "        \"\"\"\n",
    "        a, b = min(interval), max(interval)\n",
    "        # c is the squared inverse of the golden ratio\n",
    "        c, d, e = (3 - 5**0.5) / 2, 0, 0\n",
    "        # eps is approximately the square root of relative machine precision.\n",
    "        tol = sys.float_info.epsilon**0.25\n",
    "        eps = tol**2\n",
    "        # the smallest 1.000... > 1 : tol1 = 1 + eps**2\n",
    "        v = w = x = a + c * (b - a)\n",
    "        fv = fw = fx = fun(x, **kwargs)\n",
    "        # main loop starts here\n",
    "        while True:\n",
    "            m = (a + b) / 2\n",
    "            tol1 = eps * abs(x) + tol / 3\n",
    "            tol2 = tol1 * 2\n",
    "            # check stopping criterion\n",
    "            if abs(x - m) <= tol2 - (b - a) / 2:\n",
    "                break\n",
    "            p = q = r = 0\n",
    "            # fit parabola\n",
    "            if abs(e) > tol1:\n",
    "                r = (x - w) * (fx - fv)\n",
    "                q = (x - v) * (fx - fw)\n",
    "                p = (x - v) * q - (x - w) * r\n",
    "                q = (q - r) * 2\n",
    "                if q > 0:\n",
    "                    p *= -1\n",
    "                else:\n",
    "                    q *= -1\n",
    "                r, e = e, d\n",
    "            if abs(p) >= abs(0.5 * q * r) or p <= q * (a-x) or p >= q * (b-x):\n",
    "                # a golden-section step\n",
    "                e = (b if x < m else a) - x\n",
    "                d = c * e\n",
    "            else:\n",
    "                # a parabolic-interpolation step\n",
    "                d = p / q\n",
    "                u = x + d\n",
    "                # f must not be evaluated too close to ax or bx\n",
    "                if u - a < tol2 or b - u < tol2:\n",
    "                    d = tol1 if x < m else -tol1\n",
    "            # f must not be evaluated too close to x\n",
    "            if abs(d) >= tol1:\n",
    "                u = x + d\n",
    "            elif d > 0:\n",
    "                u = x + tol1\n",
    "            else:\n",
    "                u = x - tol1\n",
    "            fu = fun(u, **kwargs)\n",
    "            # update  a, b, v, w, and x\n",
    "            if fu <= fx:\n",
    "                if u < x:\n",
    "                    b = x\n",
    "                else:\n",
    "                    a = x\n",
    "                v, fv, w, fw, x, fx = w, fw, x, fx, u, fu\n",
    "            else:\n",
    "                if u < x:\n",
    "                    a = u\n",
    "                else:\n",
    "                    b = u\n",
    "                if fu <= fw or w == x:\n",
    "                    v, fv, w, fw = w, fw, u, fu\n",
    "                elif fu <= fv or v == x or v == w:\n",
    "                    v, fv = u, fu\n",
    "        # end of main loop\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165adaa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:05.431004Z",
     "iopub.status.busy": "2025-12-29T18:09:05.430626Z",
     "iopub.status.idle": "2025-12-29T18:09:05.497072Z",
     "shell.execute_reply": "2025-12-29T18:09:05.496143Z"
    },
    "papermill": {
     "duration": 0.093271,
     "end_time": "2025-12-29T18:09:05.499015",
     "exception": false,
     "start_time": "2025-12-29T18:09:05.405744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "from stochastic.processes.continuous import FractionalBrownianMotion as fbm\n",
    "from math import pi, gamma\n",
    "from scipy import fft, stats\n",
    "\n",
    "\n",
    "class HurstIndexSolver(AddMethods):\n",
    "\n",
    "    def __init__(self):\n",
    "        None\n",
    "\n",
    "    def __FitCurve(self, Scale: list, StatisticModel: list,\n",
    "                   method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Fitting scale ~ statisticModel in a log-log plot.\n",
    "        \"\"\"\n",
    "        Scale = np.log10(np.array([Scale]))\n",
    "        Scale = np.vstack([Scale, np.ones(len(StatisticModel))]).T\n",
    "        if method == 'L2':\n",
    "            # slope = np.polyfit(np.log10(Cm), np.log10(AM), 1)[0]\n",
    "            slope, c = np.linalg.lstsq(\n",
    "                Scale,\n",
    "                np.log10(StatisticModel),\n",
    "                rcond=-1\n",
    "            )[0]\n",
    "        elif method == 'L1':\n",
    "            slope, c = super().OLE_linprog(\n",
    "                Scale,\n",
    "                np.array([np.log10(StatisticModel)]).T\n",
    "            )\n",
    "            slope = slope[0]\n",
    "        # slope = np.polyfit(np.log(Scale), np.log(StatisticModel), deg = 1)[0]\n",
    "        return slope\n",
    "\n",
    "    def EstHurstClustering(self, ts, order: float, minimal=10,\n",
    "                           method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Hurst exponent using Clustering Method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 10.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Absolute Moments Method (AM).\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n",
    "        [J]. 2021.\n",
    "\n",
    "        written by z.q.feng at 2022.09.05\n",
    "        \"\"\"\n",
    "        N = len(ts)\n",
    "        # make sure m is large and (N / m) is large\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "\n",
    "        ts = ts[N - OptN:]\n",
    "        # The mean for series\n",
    "        Avg = np.mean(ts)\n",
    "\n",
    "        CM = []\n",
    "        for m in M:\n",
    "            k = OptN // m\n",
    "            # remove the redundant data at the begin\n",
    "            # each row is a subseries with N m\n",
    "            Xsub = np.reshape(ts, [k, m])\n",
    "            # mean of each suseries\n",
    "            Xm = np.mean(Xsub, axis=1)\n",
    "            # order == 1 : Absolute Moments Method\n",
    "            # order == 2 : Aggregated Variance Method\n",
    "            CM.append(np.mean(abs(Xm - Avg)**order))\n",
    "\n",
    "        slope = self.__FitCurve(M, CM, method=method)\n",
    "        return slope / order + 1\n",
    "\n",
    "    def EstHurstAbsoluteMoments(self, ts, minimal=20, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Hurst exponent using Cluster Method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 10.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Absolute Moments Method (AM).\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n",
    "        [J]. 2021.\n",
    "\n",
    "        written by z.q.feng at 2022.09.05\n",
    "        \"\"\"\n",
    "        N = len(ts)\n",
    "        # make sure m is large and (N / m) is large\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "\n",
    "        ts = ts[N - OptN:]\n",
    "        # The mean for series\n",
    "        Avg = np.mean(ts)\n",
    "\n",
    "        AM = []\n",
    "        for m in M:\n",
    "            k = OptN // m\n",
    "            # remove the redundant data at the begin\n",
    "            # each row is a subseries with N m\n",
    "            Xsub = np.reshape(ts, [k, m])\n",
    "            # mean of each suseries\n",
    "            Xm = np.mean(Xsub, axis=1)\n",
    "            AM.append(np.linalg.norm(Xm - Avg, 1) / len(Xm))\n",
    "\n",
    "        slope = self.__FitCurve(M, AM, method=method)\n",
    "        return slope + 1\n",
    "\n",
    "    def EstHurstAggregateVariance(self, ts, minimal=12, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Hurst exponent using Cluster Method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 10.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Aggregate Variance Method (AV).\n",
    "\n",
    "        Reference\n",
    "        ---------\n",
    "        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n",
    "        [J]. 2021.\n",
    "\n",
    "        written by z.q.feng at 2022.09.05\n",
    "        \"\"\"\n",
    "        N = len(ts)\n",
    "        # The mean for series\n",
    "        # Avg = np.mean(ts)\n",
    "\n",
    "        # make sure m is large and (N / m) is large\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "\n",
    "        AV = []\n",
    "        for m in M:\n",
    "            k = OptN // m\n",
    "            # remove the redundant data at the begin\n",
    "            # each row is a subseries with N m\n",
    "            Xsub = np.reshape(ts[N - OptN:], [k, m])\n",
    "            # mean of each suseries\n",
    "            Xm = np.mean(Xsub, axis=1)\n",
    "            AV.append(np.var(Xm, ddof=0))\n",
    "            # AV.append(np.var(Xm - Avg, ddof=1))\n",
    "\n",
    "        slope = self.__FitCurve(M, AV, method=method)\n",
    "        return slope / 2 + 1\n",
    "\n",
    "    def EstHurstDFAnalysis(self, ts, minimal=12, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        DFA Calculate the Hurst exponent using DFA analysis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 10.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Detrended Fluctuation Analysis (DFA).\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] C.-K.Peng et al. (1994) Mosaic organization of DNA nucleotides,\n",
    "        Physical Review E 49(2), 1685-1689.\n",
    "        [2] R.Weron (2002) Estimating long range dependence: finite sample\n",
    "        properties and confidence intervals, Physica A 312, 285-299.\n",
    "\n",
    "        Written by z.q.feng (2022.09.23).\n",
    "        Based on dfa.m orginally written by afal Weron (2011.09.30).\n",
    "        \"\"\"\n",
    "        DF = []\n",
    "        N = len(ts) + 1\n",
    "        y = np.concatenate([[0], np.cumsum(ts)])\n",
    "\n",
    "        OptN = super().findOptN(len(ts), minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "\n",
    "        for m in M:\n",
    "            k = OptN // m\n",
    "            Y = np.reshape(y[N - OptN:], [m, k], order='F')\n",
    "            F = np.copy(Y)\n",
    "            # t = 1, 2, ..., m\n",
    "            t = np.linspace(1, m, m)\n",
    "            for i in range(k):\n",
    "                p = np.polyfit(t, Y[:, i], 1)\n",
    "                F[:, i] = Y[:, i] - t * p[0] - p[1]\n",
    "            DF.append(np.mean(np.std(F)))\n",
    "\n",
    "        slope = self.__FitCurve(M, DF, method=method)\n",
    "        return slope\n",
    "\n",
    "    def __getBox(self, j: int) -> int:\n",
    "        \"\"\"\n",
    "        [2^{(j-1)/4}] for j in (11, 12, 13, ...) if k > 4\n",
    "        \"\"\"\n",
    "        if j < 5:\n",
    "            return j\n",
    "        else:\n",
    "            return int(2 ** ((j + 5) / 4))\n",
    "\n",
    "    def EstHurstHiguchi(self, ts, minimal=11, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Hurst exponent using Higuchi Method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using Higuchi Method (HM).\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Higuchi T. Approach to an irregular time series on the basis of\n",
    "            the fractal theory[J]. Physica D: Nonlinear Phenomena, 1988, 31(2):\n",
    "            277-283.\n",
    "        \"\"\"\n",
    "        N = len(ts) + 1\n",
    "        Lm, Cm = [], []\n",
    "        # FGN --diff--> Gaussian\n",
    "        Y = np.concatenate([[0], np.cumsum(ts)])\n",
    "\n",
    "        for j in range(1, minimal):\n",
    "            Lk = []\n",
    "            m = self.__getBox(j)\n",
    "            Cm.append(m)\n",
    "            k = N // m\n",
    "            Xsub = np.reshape(Y[N % m:], [k, m])\n",
    "            for i in range(1, k):\n",
    "                Lk.append(abs(Xsub[i] - Xsub[i - 1]))\n",
    "            # Lm = np.mean(np.array(Lk), axis=0) * (N - 1) / k / k\n",
    "            Lm.append(np.mean(Lk) * (N - 1) / m / m)\n",
    "\n",
    "        slope = self.__FitCurve(Cm, Lm, method=method)\n",
    "        return slope + 2\n",
    "\n",
    "    def EstHurstRegrResid(self, ts, minimal=20, method='L2') -> float:\n",
    "        '''\n",
    "        Variance of the regression residuals (VRR) for Hurst Index.\n",
    "        '''\n",
    "        Sigma = []\n",
    "        N = len(ts)\n",
    "\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)[:-3]\n",
    "\n",
    "        for m in M:\n",
    "            res = []\n",
    "            k = OptN // m\n",
    "            t = np.linspace(1, m, m)\n",
    "            X = np.reshape(ts[N - OptN:], [k, m])\n",
    "            for i in range(k):\n",
    "                Y = np.cumsum(X[i] - np.mean(X[i]))\n",
    "                a, b = np.polyfit(t, Y, 1)\n",
    "                res.append(np.std(Y - a * t - b, ddof=1))\n",
    "            Sigma.append(np.mean(res))\n",
    "\n",
    "        slope = self.__FitCurve(M, Sigma, method=method)\n",
    "        return slope\n",
    "\n",
    "    def __HalfSeries(self, s: list, n: int) -> list:\n",
    "        X = []\n",
    "        for i in range(0, len(s) - 1, 2):\n",
    "            X.append((s[i] + s[i + 1]) / 2)\n",
    "        # if length(s) is odd\n",
    "        if len(s) % 2 != 0:\n",
    "            X.append(s[-1])\n",
    "            n = (n - 1) // 2\n",
    "        else:\n",
    "            n = n // 2\n",
    "        return [np.array(X), n]\n",
    "\n",
    "    def RS4Hurst(self, ts: np.ndarray, minimal=4, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        RS Analysis for solve the Hurst exponent.\n",
    "        \"\"\"\n",
    "        ts = np.array(ts)\n",
    "        # N is use for storge the length sequence\n",
    "        N, RS, n = [], [], len(ts)\n",
    "        while (True):\n",
    "            N.append(n)\n",
    "            # Calculate the average value of the series\n",
    "            m = np.mean(ts)\n",
    "            # Construct mean adjustment sequence\n",
    "            mean_adj = ts - m\n",
    "            # Construct cumulative deviation sequence\n",
    "            cumulative_dvi = np.cumsum(mean_adj)\n",
    "            # Calculate sequence range\n",
    "            srange = max(cumulative_dvi) - min(cumulative_dvi)\n",
    "            # Calculate the unbiased standard deviation of this sequence\n",
    "            unbiased_std_dvi = np.std(ts, ddof=1)\n",
    "            # Calculate the rescaled range of this sequence\n",
    "            RS.append(srange / unbiased_std_dvi)\n",
    "            # While n < 2 then break\n",
    "            if n < minimal:\n",
    "                break\n",
    "            # Rebuild this sequence by half length\n",
    "            ts, n = self.__HalfSeries(ts, n)\n",
    "        # Get Hurst-index by fit log(RS)~log(n)\n",
    "        slope = np.polyfit(np.log(N), np.log(RS), deg = 1)[0]\n",
    "        # slope = self.__FitCurve(N, RS, method=method)\n",
    "        return slope\n",
    "\n",
    "    def EstHurstRSAnalysis(self, ts, minimal=20, IsRandom=False,\n",
    "                           method='L2') -> float:\n",
    "        '''\n",
    "        RS Calculate the Hurst exponent using DFA analysis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 50.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Rescaled Range Analysis (DFA).\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] H.-H.Amjad et al. (2021) Comparison of Hurst exponent estimation\n",
    "        methods, Physical Review E 49(2), 1685-1689.\n",
    "        [2] R.Weron (2002) Estimating long range dependence: finite sample\n",
    "        properties and confidence intervals, Physica A 312, 285-299.\n",
    "        [3] E.E.Peters (1994) Fractal Market Analysis, Wiley.\n",
    "        [4] A.A.Anis, E.H.Lloyd (1976) The expected value of the adjusted\n",
    "        rescaled Hurst range of independent normal summands, Biometrica 63,\n",
    "        283-298.\n",
    "\n",
    "        Written by z.q.feng (2022.09.23).\n",
    "        '''\n",
    "        N = len(ts)\n",
    "\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "\n",
    "        # M is use for storge the length sequence\n",
    "        RSe, ERS_AL = [], []\n",
    "        for m in M:\n",
    "            RSm = []\n",
    "            k = OptN // m\n",
    "            X = np.reshape(ts[N - OptN:], [k, m])\n",
    "            for Xm in X:\n",
    "                # Calculate the average value of the sub-series\n",
    "                Em = np.mean(Xm)\n",
    "                # Construct mean adjustment sequence\n",
    "                mean_adj = Xm - Em\n",
    "                # Construct cumulative deviation sequence\n",
    "                cumulative_dvi = np.cumsum(mean_adj)\n",
    "                # Calculate sequence range\n",
    "                srange = max(cumulative_dvi) - min(cumulative_dvi)\n",
    "                # Calculate the unbiased standard deviation of this sequence\n",
    "                unbiased_std_dvi = np.std(mean_adj, ddof=1)\n",
    "                # Calculate the rescaled range of this sequence under n length\n",
    "                RSm.append(srange / unbiased_std_dvi)\n",
    "            RSe.append(np.mean(RSm))\n",
    "\n",
    "        # Compute Anis-Lloyd[4] and Peters[3] corrected theoretical E(R/S)\n",
    "        for m in M:\n",
    "            # (see [2] for details)\n",
    "            K = np.linspace(1, m - 1, m - 1)\n",
    "            ratio = (m - 0.5) / m * np.sum(((-K + m) / K)**0.5)\n",
    "            if m > 340:\n",
    "                ERS_AL.append(ratio / (0.5 * pi * m)**0.5)\n",
    "            else:\n",
    "                ERS_AL.append((gamma(0.5*(m-1))*ratio)/(gamma(0.5*m)*pi**0.5))\n",
    "        # see Peters[3] page 66 eq 5.1\n",
    "        ERS = (0.5 * pi * np.array(M))**0.5\n",
    "\n",
    "        RSe, ERS_AL = np.array(RSe), np.array(ERS_AL)\n",
    "        RS = RSe - ERS_AL + ERS if IsRandom else RSe\n",
    "\n",
    "        slope = self.__FitCurve(M, RS, method=method)\n",
    "        return slope\n",
    "\n",
    "    def EstHurstRSAnalysis2(self, ts, minimal=20, method='L2') -> float:\n",
    "        '''\n",
    "        RS Calculate the Hurst exponent using DFA analysis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        minimal: The box sizes that the sample is divided into, default as 50.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series X using\n",
    "        Rescaled Range Analysis (DFA).\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] H.-H.Amjad et al. (2021) Comparison of Hurst exponent estimation\n",
    "        methods, Physical Review E 49(2), 1685-1689.\n",
    "        [2] R.Weron (2002) Estimating long range dependence: finite sample\n",
    "        properties and confidence intervals, Physica A 312, 285-299.\n",
    "        [3] E.E.Peters (1994) Fractal Market Analysis, Wiley.\n",
    "        [4] A.A.Anis, E.H.Lloyd (1976) The expected value of the adjusted\n",
    "        rescaled Hurst range of independent normal summands, Biometrica 63,\n",
    "        283-298.\n",
    "\n",
    "        Written by z.q.feng (2022.09.23).\n",
    "        '''\n",
    "        N = len(ts) + 1\n",
    "\n",
    "        OptN = super().findOptN(N, minimal=minimal)\n",
    "        M = super().Divisors(OptN, minimal=minimal)\n",
    "        y = np.concatenate([[0], np.cumsum(ts)])\n",
    "\n",
    "        # M is use for storge the length sequence\n",
    "        RSe = []\n",
    "        for m in M:\n",
    "            RSm = []\n",
    "            k = OptN // m\n",
    "            X = np.reshape(y[N - OptN:], [k, m])\n",
    "            for i in range(k):\n",
    "                # Calculate sequence range\n",
    "                srange = max(X[i]) - min(X[i])\n",
    "                # Calculate the unbiased standard deviation of this sequence\n",
    "                unbiased_std_dvi = np.std(ts[i*m:(i+1)*m], ddof=0)\n",
    "                # Calculate the rescaled range of this sequence under n length\n",
    "                RSm.append(srange / unbiased_std_dvi)\n",
    "            RSe.append(np.mean(RSm))\n",
    "\n",
    "        slope = self.__FitCurve(M, RSe, method=method)\n",
    "        return slope\n",
    "\n",
    "    def EstHurstPeriodogram(self, ts, cutoff=0.3, method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts     : Time series.\n",
    "        cutoff : Level of low Fourier frequencies, default as 0.5.\n",
    "        method : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent H of a time series ts estimated using the\n",
    "        Geweke-Porter-Hudak (GPH, 1983) spectral estimator for periods\n",
    "        lower than max(period)^CUTOFF, where CUTOFF=0.5.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] J.Geweke, S.Porter-Hudak (1983) The estimation and application of\n",
    "        long memory time series models, Journal of Time Series Analysis 4,\n",
    "        221-238.\n",
    "        [2] R.Weron (2002) Estimating long range dependence: finite sample\n",
    "        properties and confidence intervals, Physica A 312, 285-299.\n",
    "\n",
    "        Written by z.q.feng (2022.09.21).\n",
    "        \"\"\"\n",
    "        N = len(ts)\n",
    "        # Compute the Fourier transform of the data\n",
    "        # Remove the first component of Y, which is simply the sum of the data\n",
    "        Y = fft.fft(ts)[1:N // 2 + 1]\n",
    "        # Define the frequencies\n",
    "        freq = np.linspace(1 / N, 0.5, N // 2)\n",
    "        # Find the low Fourier frequencies\n",
    "        index = freq < 1 / N ** cutoff\n",
    "        # The periodogram is deﬁned as\n",
    "        IL = 4 * np.sin(freq[index] / 2) ** 2\n",
    "        # Compute the power as the squared absolute value of Y\n",
    "        # A plot of power versus frequency is the 'periodogram'\n",
    "        power = abs(Y[index]) ** 2 / N\n",
    "        slope = self.__FitCurve(IL, power, method=method)\n",
    "        return 0.5 - slope / 2\n",
    "\n",
    "    def EstHurstAWC(self, ts, wavelet=\"db24\", wavemode=\"periodization\",\n",
    "                    method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts      : Time series.\n",
    "        wavelet : Wavelet type, default as \"db24\".\n",
    "        wavemode: The discrete wavelet transform extension mode,\n",
    "                  default as \"periodization\".\n",
    "        method  : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent H of a time series ts estimated using the\n",
    "        Average Wavelet Coefficient (AWC) method.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] I.Simonsen, A.Hansen, O.Nes (1998) Determination of the Hurst\n",
    "        exponent by use of wavelet transforms, Physical Review E 58, 2779-2787.\n",
    "        [2] I.Simonsen (2003) Measuring anti-correlations in the Nordic\n",
    "        electricity spot market by wavelets, Physica A 322, 597-606.\n",
    "        [3] R.Weron, I.Simonsen, P.Wilman (2004) Modeling highly volatile and\n",
    "        seasonal markets: evidence from the Nord Pool electricity market, in\n",
    "        \"The Application of Econophysics\", ed. H. Takayasu, Springer, 182-191.\n",
    "        [4] R.Weron (2006) Modeling and Forecasting Electricity Loads and\n",
    "        Prices: A Statistical Approach, Wiley, Chichester.\n",
    "\n",
    "        Written by z.q.feng (2022.09.24).\n",
    "        Based on function awc_hurst.m originally written by Rafal Weron\n",
    "        (2014.06.21).\n",
    "        \"\"\"\n",
    "        # Do not allow for too large values of N\n",
    "        N = int(np.floor(np.log2(len(ts)))) - 1\n",
    "\n",
    "        # Daubechies wavelet of order 24\n",
    "        # Set the DWT mode to periodization, see pywt.Modes for details\n",
    "        # coeffs contains one Approximation and N Detail-Coefficients\n",
    "        coeffs = pywt.wavedec(ts, wavelet=wavelet, mode=wavemode, level=N)\n",
    "        sc, awc = [], []\n",
    "        for i in range(1, N - 1):\n",
    "            # Get the Detail-Coefficients\n",
    "            cD = coeffs[-i]\n",
    "            sc.append(2 ** i)\n",
    "            # Compute the AWC statistics\n",
    "            awc.append(np.mean(abs(cD)))\n",
    "        # Level value of N is too high\n",
    "        # sc, awc = sc[:-1], awc[:-1]\n",
    "        slope = self.__FitCurve(sc, awc, method=method)\n",
    "        return slope + 0.5\n",
    "\n",
    "    def EstHurstVVL(self, ts, wavelet=\"haar\", wavemode=\"periodization\",\n",
    "                    method='L2') -> float:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts      : Time series. Be careful of N >= 2^15.\n",
    "        wavelet : Wavelet type, default as \"db24\".\n",
    "        wavemode: The discrete wavelet transform extension mode,\n",
    "                  default as \"periodization\".\n",
    "        method  : The method to fit curve, default as minimal l2-norm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent H of a time series ts estimated using the\n",
    "        Variance Versus Level Method (VVL) using wavelets.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation\n",
    "        methods[J]. 2021.\n",
    "\n",
    "        Written by z.q.feng (2022.09.24).\n",
    "        \"\"\"\n",
    "        # Do not allow for too large values of N\n",
    "        N = int(np.floor(np.log2(len(ts))))\n",
    "        # Haar wavelet\n",
    "        # Set the DWT mode to periodization, see pywt.Modes for details\n",
    "        # coeffs contains one Approximation and N Detail-Coefficients\n",
    "        coeffs = pywt.wavedec(ts, wavelet, level=N, mode=wavemode)\n",
    "        sc, vvl = [], []\n",
    "        for i in range(1, N - 1):\n",
    "            # Get the Detail-Coefficients\n",
    "            cD = coeffs[-i]\n",
    "            sc.append(2 ** i)\n",
    "            # Compute the VVL statistics\n",
    "            vvl.append(np.var(abs(cD), ddof=1))\n",
    "        slope = self.__FitCurve(sc, vvl, method=method)\n",
    "        return (slope + 1) / 2\n",
    "\n",
    "    def EstHurstLocalWhittle(self, ts: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Semiparametric Gaussian Estimation via Fast Fourier transform.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts        : Time series.\n",
    "        iter_count: Accuracy for estimation degree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series ts.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Robinson, P. M.(1995). \"Gaussian semiparametric estimation\n",
    "            of long-range dependence\". The Annals of Statistics. 23(5):\n",
    "            1630–1661. doi:10.1214/aos/1176324317.\n",
    "\n",
    "        Written by z.q.feng (2023.06.07).\n",
    "        \"\"\"\n",
    "        n = len(ts)\n",
    "        m = n // 2  # Less than n / 2\n",
    "        w = fft.fft(ts)[1:m + 1]  # Fast Fourier transform\n",
    "        Periodogram = abs(w)**2\n",
    "        Freqs = np.linspace(1, m + 1, m) / n  # Frequences\n",
    "\n",
    "        def Whittle_target(H, **kwargs):\n",
    "            \"\"\"\n",
    "            Target function to minimize in Local Whittle method.\n",
    "            \"\"\"\n",
    "            Freqs = kwargs[\"Freqs\"]\n",
    "            Periodogram = kwargs[\"Periodogram\"]\n",
    "            gH = np.mean(Freqs**(2 * H - 1) * Periodogram)\n",
    "            rH = np.log(gH) - (2 * H - 1) * np.mean(np.log(Freqs))\n",
    "            return rH\n",
    "\n",
    "        H = super().LocalMin(\n",
    "                Whittle_target,\n",
    "                [0.001, 0.999],\n",
    "                Periodogram=Periodogram,\n",
    "                Freqs=Freqs\n",
    "            )\n",
    "\n",
    "        return H\n",
    "\n",
    "    def EstHurstLSSD(self, ts: np.ndarray, max_scale: int, p=6, q=0,\n",
    "                     eps=1e-6) -> float:\n",
    "        \"\"\"\n",
    "        Least Squares based on Standard Deviation. The fitting error is\n",
    "        constructed by sample standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts        : Time series.\n",
    "        max_scale : Maximum aggregation scale(>= length / 10).\n",
    "        p         : Parameter used to determine the weights.\n",
    "        q         : Parameter used to determine the penalty factor.\n",
    "        eps       : Accuracy for estimation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series ts.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Koutsoyiannis D (2003) Climate change, the Hurst phenomenon, and\n",
    "            hydrological statistics. Hydrological Sciences Journal 48(1):3–24.\n",
    "            doi:10.1623/hysj.48.1.3.43481.\n",
    "        [2] Tyralis H, Koutsoyiannis D (2011) Simultaneous estimation of the\n",
    "            parameters of the Hurst-Kolmogorov stochastic process. Stochastic\n",
    "            Environmental Research & Risk Assessment 25(1):21–33.\n",
    "            doi:10.1007/s004770100408x.\n",
    "\n",
    "        Written by z.q.feng (2023.06.07).\n",
    "        \"\"\"\n",
    "        n = len(ts)\n",
    "        # This maximum value was chosen so that VarSeq can be estimated\n",
    "        # from at least 10 data values.\n",
    "        max_scale = min(max_scale, n // 10 + 1)\n",
    "        stdSeq = np.linspace(1, max_scale, max_scale)\n",
    "        # {1, 2, ..., max_scale}\n",
    "        kscale = np.linspace(1, max_scale, max_scale).astype(int)\n",
    "        # Unbiased Stadnard Deviation of each sample\n",
    "        for scale in kscale:\n",
    "            sample = np.sum(\n",
    "                        np.reshape(ts[n % scale:], [n // scale, scale]),\n",
    "                        axis=1\n",
    "                        )\n",
    "            stdSeq[scale - 1] = np.std(sample, ddof=1)\n",
    "\n",
    "        def LSSDIterFun(H, **kwargs):\n",
    "            \"\"\"\n",
    "            A constructive mapping in LSSD method. Each improved estimate can\n",
    "            reduce the fitting error and continues this way until convergence.\n",
    "            \"\"\"\n",
    "            n = kwargs[\"n\"]\n",
    "            p = kwargs[\"p\"]\n",
    "            q = kwargs[\"q\"]\n",
    "            kscale = kwargs[\"kscale\"]\n",
    "            stdSeq = kwargs[\"stdSeq\"]\n",
    "            f = n / kscale\n",
    "            kp = kscale**p\n",
    "            logk = np.log(kscale)\n",
    "            # eq.A.3 in Ref[1]\n",
    "            a1 = np.sum(1 / kp)\n",
    "            a2 = np.sum(logk / kp)\n",
    "            # eq.12 in Ref[2]\n",
    "            ckH = ((f - f**(2 * H - 1)) / (f - 0.5))**0.5\n",
    "            # eq.A.5 in Ref[1]\n",
    "            dkH = logk + np.log(f) / (1 - f**(2 - 2 * H))\n",
    "            # eq.A.4 in Ref[1]\n",
    "            aH_1 = np.sum(dkH / kp)\n",
    "            aH_2 = np.sum(dkH * logk / kp)\n",
    "            bH_1 = np.sum((np.log(stdSeq) - np.log(ckH)) / kp)\n",
    "            bH_2 = np.sum(dkH * (np.log(stdSeq) - np.log(ckH)) / kp)\n",
    "            g1 = a1 * bH_2 - aH_1 * bH_1\n",
    "            g2 = a1 * aH_2 - aH_1 * a2\n",
    "            return (g1 if q == 0 else (g1 - a1 * H**q)) / g2\n",
    "\n",
    "        # Solving the fixed point\n",
    "        H = super().FixedPointSolver(\n",
    "                LSSDIterFun, 0.5,\n",
    "                n=n, p=p, q=q,\n",
    "                kscale=kscale,\n",
    "                stdSeq=stdSeq\n",
    "            )\n",
    "\n",
    "        return H\n",
    "\n",
    "    def EstHurstLSV(self, ts: np.ndarray, max_scale: int, p=6, q=0) -> float:\n",
    "        \"\"\"\n",
    "        Least Squares based on Variance. The fitting error is constructed by\n",
    "        sample variances.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts        : Time series.\n",
    "        max_scale : Maximum aggregation scale(>= length / 10).\n",
    "        p         : Parameter used to determine the weights.\n",
    "        q         : Parameter used to determine the penalty factor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The Hurst exponent of time series ts.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Tyralis H, Koutsoyiannis D (2011) Simultaneous estimation of the\n",
    "            parameters of the Hurst-Kolmogorov stochastic process. Stochastic\n",
    "            Environmental Research & Risk Assessment 25(1):21–33.\n",
    "            doi:10.1007/s004770100408x.\n",
    "\n",
    "        Written by z.q.feng (2023.06.07).\n",
    "        \"\"\"\n",
    "        n = len(ts)\n",
    "        # This maximum value was chosen so that VarSeq can be estimated\n",
    "        # from at least 10 data values.\n",
    "        max_scale = min(max_scale, n // 10 + 1)\n",
    "        varSeq = np.linspace(1, max_scale, max_scale)\n",
    "        # {1, 2, ..., max_scale}\n",
    "        kscale = np.linspace(1, max_scale, max_scale).astype(int)\n",
    "        # Variance of each sample\n",
    "        for scale in kscale:\n",
    "            sample = np.sum(\n",
    "                np.reshape(ts[n % scale:], [n // scale, scale]),\n",
    "                axis=1\n",
    "            )\n",
    "            varSeq[scale - 1] = np.var(sample, ddof=1)\n",
    "\n",
    "        def LSV_target(H: float, **kwargs) -> float:\n",
    "            \"\"\"\n",
    "            Target function to minimize in LSV method.\n",
    "            \"\"\"\n",
    "            n = kwargs[\"n\"]\n",
    "            p = kwargs[\"p\"]\n",
    "            q = kwargs[\"q\"]\n",
    "            kscale = kwargs[\"kscale\"]\n",
    "            varSeq = kwargs[\"varSeq\"]\n",
    "            f = n / kscale\n",
    "            kp = kscale**p\n",
    "            # Left side of eq.22 in Ref[1]\n",
    "            d1 = np.sum(varSeq**2 / kp)\n",
    "            # eq.17 in Ref[1]\n",
    "            ckH = (f - f**(2 * H - 1)) / (f - 1)\n",
    "            # eq.20 in Ref[1]\n",
    "            a1H = np.sum((ckH**2 * kscale**(4 * H)) / kp)\n",
    "            a2H = np.sum((ckH * kscale**(2 * H) * varSeq) / kp)\n",
    "            r = d1 - a2H**2 / (a1H if a1H > 1e-8 else 1e-8)\n",
    "            # In Ref[1], to avoid values of sigma tending to infinity\n",
    "            # when H->1, penalty factor [H^(q+1)]/(q+1) for a high q(near 50)\n",
    "            # is added to target function, see eq.24.\n",
    "            return r if q == 0 else r + H**(q + 1) / (q + 1)\n",
    "\n",
    "        # TODO: Choose an appropriate optimize algorithm.\n",
    "        H = super().LocalMin(\n",
    "                LSV_target,\n",
    "                [0.001, 0.999],\n",
    "                n=n, p=p, q=q,\n",
    "                kscale=kscale,\n",
    "                varSeq=varSeq\n",
    "            )\n",
    "\n",
    "        return H\n",
    "\n",
    "    def __GHESample(self, ts, tau):\n",
    "        \"\"\"\n",
    "        Generates sample sequence as |X(t+τ)-X(t)| for t in {0, 1, ..., N-τ-1}.\n",
    "        \"\"\"\n",
    "        return np.abs(ts[tau:] - ts[:-tau])\n",
    "\n",
    "    def EstHurstGHE(self, ts, q=2, method=\"L2\"):\n",
    "        \"\"\"\n",
    "        Generalized Hurst Exponent. Estimated the Hurst exponent by using the\n",
    "        qth-order moments of the distribution of the increments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts : Time sequence.\n",
    "        q  : Moments order.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent of the time sequence\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Albert-László Barabási and Tamás Vicsek. Multifractality of self-\n",
    "            affine fractals. Physical review A, 44(4):2730, 1991.\n",
    "        [2] Tiziana Di Matteo, Tomaso Aste, and Michel M Dacorogna. Scaling\n",
    "            behaviors in diﬀerently developed markets. Physica A: statistical\n",
    "            mechanics and its applications, 324(1-2):183–188, 2003.\n",
    "        [3] A Gómez-Águila, JE Trinidad-Segovia, and MA Sánchez-Granero.\n",
    "            Improvement in hurst exponent estimation and its application to\n",
    "            ﬁnancial markets. Financial Innovation, 8(1):1–21, 2022.\n",
    "        \"\"\"\n",
    "        Y = np.concatenate([[0], np.cumsum(ts)])\n",
    "        K, Tau = [], [i for i in range(1, 5)]\n",
    "        for tau in Tau:\n",
    "            # see Ref[3] for details.\n",
    "            K.append(np.mean(self.__GHESample(Y, tau)**q))\n",
    "        # k_q(τ) \\propto τ^{qH(q)} where H(q) is the generalized Hurst exponent\n",
    "        slope = self.__FitCurve(Tau, K, method=method)\n",
    "        return np.clip( slope / q, 0.0000001, 0.99999999)\n",
    "\n",
    "    def EstHurstKS(self, ts):\n",
    "        \"\"\"\n",
    "        Kolmogorov-Smirnov Method. In most cases, existing methods use the\n",
    "        scaling behavior (a power law) of certain elements of the process,\n",
    "        this method take expected values of the equality in distribution to\n",
    "        estimate the Hurst exponent. However, equality in distribution is a\n",
    "        stronger concept than that in expected values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts : Time sequence.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent of the time sequence.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] A Gómez-Águila, JE Trinidad-Segovia, and MA Sánchez-Granero.\n",
    "            Improvement in hurst exponent estimation and its application to\n",
    "            ﬁnancial markets. Financial Innovation, 8(1):1–21, 2022.\n",
    "        [2] JL Hodges Jr. The signiﬁcance probability of the smirnov two-sample\n",
    "            test. Arkiv för matematik, 3(5):469–486, 1958.\n",
    "        \"\"\"\n",
    "        N = len(ts) + 1\n",
    "        Y = np.concatenate([[0], np.cumsum(ts)])\n",
    "        scaling_range = [2**i for i in range(int(np.log2(N)) - 2)]\n",
    "        t0 = self.__GHESample(Y, scaling_range[0])\n",
    "\n",
    "        def KS_target(H, t0):\n",
    "            # see Ref[2] for details of Kolmogorov-Smirnov test.\n",
    "            return np.sum(\n",
    "                [stats.ks_2samp(t0, self.__GHESample(Y, tau)/tau**H).statistic\n",
    "                 for tau in scaling_range[1:]]\n",
    "            )\n",
    "\n",
    "        H = super().LocalMin(KS_target, [0.001, 0.999], t0=t0)\n",
    "        return H\n",
    "\n",
    "    def EstHurstTTA(self, ts, max_scale=11, method=\"L2\"):\n",
    "        \"\"\"\n",
    "        Triangle Total Areas Method. Hurst exponent is the possibility of high\n",
    "        or low values occurrence in time sequences. Based on this fact,\n",
    "        triangle area of three samples in time sequences can be an important\n",
    "        parameter for evaluation of series values. If three samples in time\n",
    "        sequences have the same values, then the area of the mated triangle of\n",
    "        this three samples has the lowest value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts        : Time sequence.\n",
    "        max_scale : Maximum interval time for each triangle.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent of the time sequence.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Hamze Lotfalinezhad and Ali Maleki. TTA, a new approach to\n",
    "            estimate hurst exponent with less estimation error and computa-\n",
    "            tional time. Physica A: statistical mechanics and its applications,\n",
    "            553:124093, 2020.\n",
    "        [2] A Gómez-Águila and MA Sánchez-Granero. A theoretical framework\n",
    "            for the tta algorithm. Physica A: Statistical Mechanics and its\n",
    "            Applications, 582:126288, 2021.\n",
    "        \"\"\"\n",
    "        Y = np.concatenate([[0], np.cumsum(ts)])\n",
    "        # τ is the interval points between first, middle and last point\n",
    "        ST, Tau = [], [i for i in range(1, max_scale)]\n",
    "        for tau in Tau:\n",
    "            area = []\n",
    "            # Area is the half of absulote determinant of matrix below:\n",
    "            # | i,           ts[i],           1 |\n",
    "            # | i + tau,     ts[i + tau],     1 |\n",
    "            # | i + 2 * tau, ts[i + 2 * tau], 1 |\n",
    "            for i in range(0, len(Y) - 2 * tau, 2 * tau):\n",
    "                area.append(\n",
    "                    abs(Y[i + 2 * tau] - 2 * Y[i + tau] + Y[i])\n",
    "                )\n",
    "            ST.append(0.5 * tau * np.sum(area))\n",
    "        slope = self.__FitCurve(Tau, ST, method=method)\n",
    "        return slope\n",
    "\n",
    "    def EstHurstTA(self, ts, q=2, method=\"L2\"):\n",
    "        \"\"\"\n",
    "        Triangle Areas Method. the modiﬁcation is just to consider the\n",
    "        distribution of the area of the triangles, instead of the distribution\n",
    "        of the sum of the areas of all the triangles.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts : Time sequence.\n",
    "        q  : Moments order.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Hurst exponent of the time sequence.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] A Gómez-Águila and MA Sánchez-Granero. A theoretical framework\n",
    "            for the tta algorithm. Physica A: Statistical Mechanics and its\n",
    "            Applications, 582:126288, 2021.\n",
    "        \"\"\"\n",
    "        Y = np.concatenate([[0], np.cumsum(ts)])\n",
    "        ST, Tau = [], [2**i for i in range(int(np.log2(len(ts))) - 1)]\n",
    "\n",
    "        # TODO: Find the correct distribution.\n",
    "        for tau in Tau:\n",
    "            area = (0.5 * tau * abs(Y[2 * tau] - 2 * Y[tau] + Y[0]))**q\n",
    "            ST.append(area)\n",
    "\n",
    "        slope = self.__FitCurve(Tau, ST, method=method)\n",
    "\n",
    "        return slope / q - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a3de45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:05.545924Z",
     "iopub.status.busy": "2025-12-29T18:09:05.545564Z",
     "iopub.status.idle": "2025-12-29T18:09:06.002938Z",
     "shell.execute_reply": "2025-12-29T18:09:06.001807Z"
    },
    "papermill": {
     "duration": 0.483063,
     "end_time": "2025-12-29T18:09:06.004991",
     "exception": false,
     "start_time": "2025-12-29T18:09:05.521928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from stochastic.processes.noise import FractionalGaussianNoise as FGN\n",
    "# from matplotlib import rc\n",
    "# rc('text', usetex=True)\n",
    "import numpy as np\n",
    "\n",
    "HSolver = HurstIndexSolver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cefb368a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:06.052303Z",
     "iopub.status.busy": "2025-12-29T18:09:06.051700Z",
     "iopub.status.idle": "2025-12-29T18:09:06.066956Z",
     "shell.execute_reply": "2025-12-29T18:09:06.065822Z"
    },
    "papermill": {
     "duration": 0.040694,
     "end_time": "2025-12-29T18:09:06.068858",
     "exception": false,
     "start_time": "2025-12-29T18:09:06.028164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module have been implemented to compute an Hurst exponent\n",
    "Estimator using maximum of likelyhood on the periodogram.\n",
    "It is not a log regression as in Hest_Welp\n",
    "the  periodogram is the one given by the scipy library\n",
    "these functions hab been made according to whittlenew,\n",
    "wittlefunc and fspecFGN of Matlab Biyu_code\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import scipy.optimize as so\n",
    "from math import gamma\n",
    "\n",
    "\n",
    "def whittle(data):\n",
    "    \"\"\"This function compute the Hurst exponent of a signal using\n",
    "    a maximum of likelihood on periodogram\n",
    "    \"\"\"\n",
    "    nbpoints = len(data)\n",
    "    nhalfm = int((nbpoints - 1) / 2)\n",
    "    tmp = np.abs(np.fft.fft(data))\n",
    "    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n",
    "    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n",
    "    return so.fminbound(func, 0, 1)\n",
    "\n",
    "\n",
    "def whittle_t(data, pointeur, idx):\n",
    "    \"\"\"This function compute the Hurst exponent of a signal using\n",
    "    a maximum of likelihood on periodogram\n",
    "    the return happend in pointeur[idx]\n",
    "    \"\"\"\n",
    "    nbpoints = len(data)\n",
    "    nhalfm = int((nbpoints - 1) / 2)\n",
    "    tmp = np.abs(np.fft.fft(data))\n",
    "    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n",
    "    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n",
    "    pointeur[idx] = so.fminbound(func, 0, 1)\n",
    "\n",
    "\n",
    "def whittle_s(data, idx):\n",
    "    \"\"\"This function compute the Hurst exponent of a signal data[idx,:]\n",
    "    using a maximum of likelihood on welch periodogram\n",
    "    \"\"\"\n",
    "    dataidx = data[idx, :]\n",
    "    nbpoints = len(dataidx)\n",
    "    nhalfm = int((nbpoints - 1) / 2)\n",
    "    tmp = np.abs(np.fft.fft(dataidx))\n",
    "    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n",
    "    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n",
    "    return so.fminbound(func, 0, 1)\n",
    "\n",
    "\n",
    "def whittle_norm_s(data, idx):\n",
    "    \"\"\"This function compute the Hurst exponent of a signal data[idx,:]\n",
    "    using a maximum of likelihood on periodogram\n",
    "    the data are normalized which does'nt change anything !\n",
    "    \"\"\"\n",
    "    dataidx = data[idx, :]\n",
    "    nbpoints = len(dataidx)\n",
    "    datanorm = (dataidx - np.mean(dataidx)) / np.var(dataidx)\n",
    "    nhalfm = int((nbpoints - 1) / 2)\n",
    "    tmp = np.abs(np.fft.fft(datanorm))\n",
    "    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n",
    "    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n",
    "    return so.fminbound(func, 0, 1)\n",
    "\n",
    "\n",
    "def whittlefunc(hurst, gammahat, nbpoints):\n",
    "    \"\"\"This is the Whittle function\n",
    "    \"\"\"\n",
    "    gammatheo = fspec_fgn(hurst, nbpoints)\n",
    "    qml = gammahat / gammatheo\n",
    "    return 2 * (2 * np.pi / nbpoints) * np.sum(qml)\n",
    "\n",
    "\n",
    "def fspec_fgn(hest, nbpoints):\n",
    "    \"\"\"This is the spectral density of a fGN of Hurst exponent hest\n",
    "    \"\"\"\n",
    "    hhest = - ((2 * hest) + 1)\n",
    "    const = np.sin(np.pi * hest) * gamma(- hhest) / np.pi\n",
    "    nhalfm = int((nbpoints - 1) / 2)\n",
    "    dpl = 2 * np.pi * np.arange(1, nhalfm + 1) / nbpoints\n",
    "    fspec = np.ones(nhalfm)\n",
    "    for i in np.arange(0, nhalfm):\n",
    "        dpfi = np.arange(0, 200)\n",
    "        dpfi = 2 * np.pi * dpfi\n",
    "        fgi = (np.abs(dpl[i] + dpfi)) ** hhest\n",
    "        fhi = (np.abs(dpl[i] - dpfi)) ** hhest\n",
    "        dpfi = fgi + fhi\n",
    "        dpfi[0] = dpfi[0] / 2\n",
    "        dpfi = (1 - np.cos(dpl[i])) * const * dpfi\n",
    "        fspec[i] = np.sum(dpfi)\n",
    "    fspec = fspec / np.exp(2 * np.sum(np.log(fspec)) / nbpoints)\n",
    "    return fspec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af111f12",
   "metadata": {
    "papermill": {
     "duration": 0.022866,
     "end_time": "2025-12-29T18:09:06.115005",
     "exception": false,
     "start_time": "2025-12-29T18:09:06.092139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate AnDi-2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a311eaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:06.162341Z",
     "iopub.status.busy": "2025-12-29T18:09:06.161947Z",
     "iopub.status.idle": "2025-12-29T18:09:19.432025Z",
     "shell.execute_reply": "2025-12-29T18:09:19.430869Z"
    },
    "papermill": {
     "duration": 13.295718,
     "end_time": "2025-12-29T18:09:19.433972",
     "exception": false,
     "start_time": "2025-12-29T18:09:06.138254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset for task(s) [1] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n",
      "Creating a dataset for task(s) [1] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n",
      "Creating a dataset for task(s) [1] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n",
      "Creating a dataset for task(s) [1] and dimension(s) [1].\n",
      "Generating dataset for dimension 1.\n"
     ]
    }
   ],
   "source": [
    "traj_length = 25\n",
    "\n",
    "X25, Y25, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n",
    "\n",
    "traj_length = 65\n",
    "\n",
    "X65, Y65, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n",
    "\n",
    "traj_length = 125\n",
    "\n",
    "X125, Y125, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n",
    "\n",
    "traj_length = 225\n",
    "\n",
    "X225, Y225, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea34ed7",
   "metadata": {
    "papermill": {
     "duration": 0.02277,
     "end_time": "2025-12-29T18:09:19.480113",
     "exception": false,
     "start_time": "2025-12-29T18:09:19.457343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess ans predict wit RANDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af000698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:19.526956Z",
     "iopub.status.busy": "2025-12-29T18:09:19.526535Z",
     "iopub.status.idle": "2025-12-29T18:09:46.469926Z",
     "shell.execute_reply": "2025-12-29T18:09:46.468566Z"
    },
    "papermill": {
     "duration": 26.968951,
     "end_time": "2025-12-29T18:09:46.471828",
     "exception": false,
     "start_time": "2025-12-29T18:09:19.502877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bs25 = randi25.layers[0].input_shape[-1]\n",
    "bs65 = randi65.layers[0].input_shape[-1]\n",
    "bs125 = randi125.layers[0].input_shape[-1]\n",
    "bs225 = randi225.layers[0].input_shape[-1]\n",
    "#normalizing the data\n",
    "data25 = data_norm(X25[0],dim=1,task=1)\n",
    "data65 = data_norm(X65[0],dim=1,task=1)\n",
    "data125 = data_norm(X125[0],dim=1,task=1)\n",
    "data225 = data_norm(X225[0],dim=1,task=1)\n",
    "#reshaping the data\n",
    "data_rs25 = data_reshape(data25,bs=bs25,dim=1)\n",
    "data_rs65 = data_reshape(data65,bs=bs65,dim=1)\n",
    "data_rs125 = data_reshape(data125,bs=bs125,dim=1)\n",
    "data_rs225 = data_reshape(data225,bs=bs225,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length \n",
    "pred_25 = randi25.predict(data_rs25)\n",
    "pred_65 = randi65.predict(data_rs65)\n",
    "pred_125 = randi65.predict(data_rs125)\n",
    "pred_225 = randi65.predict(data_rs225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af707217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:46.519261Z",
     "iopub.status.busy": "2025-12-29T18:09:46.518896Z",
     "iopub.status.idle": "2025-12-29T18:09:46.534878Z",
     "shell.execute_reply": "2025-12-29T18:09:46.533474Z"
    },
    "papermill": {
     "duration": 0.041619,
     "end_time": "2025-12-29T18:09:46.536659",
     "exception": false,
     "start_time": "2025-12-29T18:09:46.495040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDI25 MAE : 0.3689336935147643\n",
      "RANDI65 MAE : 0.2792892699517309\n",
      "RANDI125 MAE : 0.2515728974174708\n",
      "RANDI225 MAE : 0.2558435420095921\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE with scikit-learn\n",
    "mae_randi_25 = mean_absolute_error(pred_25.flatten(), Y25[0])\n",
    "mae_randi_65 = mean_absolute_error(pred_65.flatten(), Y65[0])\n",
    "mae_randi_125 = mean_absolute_error(pred_125.flatten(), Y125[0])\n",
    "mae_randi_225 = mean_absolute_error(pred_225.flatten(), Y225[0])\n",
    "print(\"RANDI25 MAE :\", mae_randi_25)\n",
    "print(\"RANDI65 MAE :\", mae_randi_65)\n",
    "print(\"RANDI125 MAE :\", mae_randi_125)\n",
    "print(\"RANDI225 MAE :\", mae_randi_225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a21ee6",
   "metadata": {
    "papermill": {
     "duration": 0.022459,
     "end_time": "2025-12-29T18:09:46.582347",
     "exception": false,
     "start_time": "2025-12-29T18:09:46.559888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict with TA-MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a21c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:46.629788Z",
     "iopub.status.busy": "2025-12-29T18:09:46.629428Z",
     "iopub.status.idle": "2025-12-29T18:09:46.865156Z",
     "shell.execute_reply": "2025-12-29T18:09:46.863857Z"
    },
    "papermill": {
     "duration": 0.261526,
     "end_time": "2025-12-29T18:09:46.867000",
     "exception": false,
     "start_time": "2025-12-29T18:09:46.605474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 25, 1) (8000, 65, 1) (8000, 125, 1) (8000, 225, 1)\n"
     ]
    }
   ],
   "source": [
    "cumdata25 = np.array(X25[0]).reshape(-1,1,25).transpose(0,2,1)\n",
    "cumdata65 = np.array(X65[0]).reshape(-1,1,65).transpose(0,2,1)\n",
    "cumdata125 = np.array(X125[0]).reshape(-1,1,125).transpose(0,2,1)\n",
    "cumdata225 = np.array(X225[0]).reshape(-1,1,225).transpose(0,2,1)\n",
    "print(cumdata25.shape, cumdata65.shape, cumdata125.shape, cumdata225.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f085a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:46.917617Z",
     "iopub.status.busy": "2025-12-29T18:09:46.917203Z",
     "iopub.status.idle": "2025-12-29T18:09:49.957845Z",
     "shell.execute_reply": "2025-12-29T18:09:49.956966Z"
    },
    "papermill": {
     "duration": 3.066325,
     "end_time": "2025-12-29T18:09:49.959765",
     "exception": false,
     "start_time": "2025-12-29T18:09:46.893440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tamsd25 = Tamsd()(cumdata25) \n",
    "tamsd65 = Tamsd()(cumdata65) \n",
    "tamsd125 = Tamsd()(cumdata125) \n",
    "tamsd225 = Tamsd()(cumdata225) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bbeaa93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:50.007632Z",
     "iopub.status.busy": "2025-12-29T18:09:50.007222Z",
     "iopub.status.idle": "2025-12-29T18:09:50.022857Z",
     "shell.execute_reply": "2025-12-29T18:09:50.021671Z"
    },
    "papermill": {
     "duration": 0.041485,
     "end_time": "2025-12-29T18:09:50.024702",
     "exception": false,
     "start_time": "2025-12-29T18:09:49.983217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSD MAE 25 : 0.4255563381849812\n",
      "MSD MAE 65 : 0.38737881541820607\n",
      "MSD MAE 125: 0.3707245659791834\n",
      "MSD MAE 225: 0.36843718238211937\n"
     ]
    }
   ],
   "source": [
    "mae_tamsd25 = mean_absolute_error(tamsd25, Y25[0])\n",
    "mae_tamsd65 = mean_absolute_error(tamsd65, Y65[0])\n",
    "mae_tamsd125 = mean_absolute_error(tamsd125, Y125[0])\n",
    "mae_tamsd225 = mean_absolute_error(tamsd225, Y225[0])\n",
    "print(\"MSD MAE 25 :\", mae_tamsd25)\n",
    "print(\"MSD MAE 65 :\", mae_tamsd65)\n",
    "print(\"MSD MAE 125:\", mae_tamsd125)\n",
    "print(\"MSD MAE 225:\", mae_tamsd225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d04a3c",
   "metadata": {
    "papermill": {
     "duration": 0.022869,
     "end_time": "2025-12-29T18:09:50.071379",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.048510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict with HurstEE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02369669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:50.120113Z",
     "iopub.status.busy": "2025-12-29T18:09:50.119738Z",
     "iopub.status.idle": "2025-12-29T18:09:50.392510Z",
     "shell.execute_reply": "2025-12-29T18:09:50.391371Z"
    },
    "papermill": {
     "duration": 0.299097,
     "end_time": "2025-12-29T18:09:50.394142",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.095045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurstLayer MAE 25 : 0.4256281208559492\n",
      "hurstLayer MAE 65 : 0.38750861606256165\n",
      "hurstLayer MAE 125: 0.37092575493117363\n",
      "hurstLayer MAE 225: 0.3684332142623427\n"
     ]
    }
   ],
   "source": [
    "hurstLayer = HurstEE()\n",
    "tfmsd25 = hurstLayer(cumdata25).numpy() * 2\n",
    "tfmsd65 = hurstLayer(cumdata65).numpy() * 2\n",
    "tfmsd125 = hurstLayer(cumdata125).numpy() * 2\n",
    "tfmsd225 = hurstLayer(cumdata225).numpy() * 2\n",
    "mae_tfmsd25 = mean_absolute_error(np.nan_to_num(tfmsd25, nan=1.0), Y25[0])\n",
    "mae_tfmsd65 = mean_absolute_error(np.nan_to_num(tfmsd65, nan=1.0), Y65[0])\n",
    "mae_tfmsd125 = mean_absolute_error(np.nan_to_num(tfmsd125, nan=1.0), Y125[0])\n",
    "mae_tfmsd225 = mean_absolute_error(np.nan_to_num(tfmsd225, nan=1.0), Y225[0])\n",
    "\n",
    "\n",
    "print(\"hurstLayer MAE 25 :\", mae_tfmsd25)\n",
    "print(\"hurstLayer MAE 65 :\", mae_tfmsd65)\n",
    "print(\"hurstLayer MAE 125:\", mae_tfmsd125)\n",
    "print(\"hurstLayer MAE 225:\", mae_tfmsd225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d872e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:50.490861Z",
     "iopub.status.busy": "2025-12-29T18:09:50.490491Z",
     "iopub.status.idle": "2025-12-29T18:09:50.814130Z",
     "shell.execute_reply": "2025-12-29T18:09:50.812787Z"
    },
    "papermill": {
     "duration": 0.349714,
     "end_time": "2025-12-29T18:09:50.815907",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.466193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyHurstEELayer MAE 25 : 0.42562812399421524\n",
      "PyHurstEELayer MAE 65 : 0.3875086154218845\n",
      "PyHurstEELayer MAE 125: 0.3709257583864168\n",
      "PyHurstEELayer MAE 225: 0.36843322543098855\n"
     ]
    }
   ],
   "source": [
    "pyhurstLayer = PyHurstEE()\n",
    "pymsd25 = pyhurstLayer(cumdata25).numpy() * 2\n",
    "pymsd65 = pyhurstLayer(cumdata65).numpy() * 2\n",
    "pymsd125 = pyhurstLayer(cumdata125).numpy() * 2\n",
    "pymsd225 = pyhurstLayer(cumdata225).numpy() * 2\n",
    "mae_pymsd25 = mean_absolute_error(np.nan_to_num(pymsd25, nan=1.0), Y25[0])\n",
    "mae_pymsd65 = mean_absolute_error(np.nan_to_num(pymsd65, nan=1.0), Y65[0])\n",
    "mae_pymsd125 = mean_absolute_error(np.nan_to_num(pymsd125, nan=1.0), Y125[0])\n",
    "mae_pymsd225 = mean_absolute_error(np.nan_to_num(pymsd225, nan=1.0), Y225[0])\n",
    "\n",
    "\n",
    "print(\"PyHurstEELayer MAE 25 :\", mae_pymsd25)\n",
    "print(\"PyHurstEELayer MAE 65 :\", mae_pymsd65)\n",
    "print(\"PyHurstEELayer MAE 125:\", mae_pymsd125)\n",
    "print(\"PyHurstEELayer MAE 225:\", mae_pymsd225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476d54eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:50.864071Z",
     "iopub.status.busy": "2025-12-29T18:09:50.863654Z",
     "iopub.status.idle": "2025-12-29T18:09:50.868409Z",
     "shell.execute_reply": "2025-12-29T18:09:50.867307Z"
    },
    "papermill": {
     "duration": 0.030676,
     "end_time": "2025-12-29T18:09:50.870127",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.839451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "methods = ['KS', 'GHE(1)', 'GHE(3)', 'HM', 'MFDFA(2)', 'Whittle', 'TTA', 'PM', 'RS', 'db4', 'LW', 'LSSD', 'LSV', 'GHE(2)', 'TA-MSD', 'TF HurstEE', 'PyHurstEE', 'RANDI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18da958",
   "metadata": {
    "papermill": {
     "duration": 0.022936,
     "end_time": "2025-12-29T18:09:50.917798",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.894862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Estimate Hurst exponent with statistical methods (AnDi-2020 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28ece698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:09:50.965430Z",
     "iopub.status.busy": "2025-12-29T18:09:50.965019Z",
     "iopub.status.idle": "2025-12-29T18:43:28.154010Z",
     "shell.execute_reply": "2025-12-29T18:43:28.152943Z"
    },
    "papermill": {
     "duration": 2017.215418,
     "end_time": "2025-12-29T18:43:28.156292",
     "exception": false,
     "start_time": "2025-12-29T18:09:50.940874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "H25 = [[] for _ in range(len(methods))]\n",
    "H65 = [[] for _ in range(len(methods))]\n",
    "H125 = [[] for _ in range(len(methods))]\n",
    "H225 = [[] for _ in range(len(methods))]\n",
    "\n",
    "minimal = 3\n",
    "max_scale = 11\n",
    "\n",
    "for row in range(len(X25[0])):\n",
    "    x25 = np.array(X25[0][row]).reshape(1,-1)\n",
    "    ts25=x25[:,1:]-x25[:,:-1]\n",
    "    ts25 = ts25 / np.std(ts25)\n",
    "    x65 = np.array(X65[0][row]).reshape(1,-1)\n",
    "    ts65=x65[:,1:]-x65[:,:-1]\n",
    "    ts65 = ts65 / np.std(ts65)\n",
    "    x125 = np.array(X125[0][row]).reshape(1,-1)\n",
    "    ts125 = x125[:,1:]-x125[:,:-1]\n",
    "    ts125 = ts125 / np.std(ts125)\n",
    "    x225 = np.array(X225[0][row]).reshape(1,-1)\n",
    "    ts225 = x225[:,1:]-x225[:,:-1]\n",
    "    ts225 = ts225 / np.std(ts225)\n",
    "    \n",
    "    H25[0].append(HSolver.EstHurstKS(ts25[0]) *2)\n",
    "    H25[1].append(HSolver.EstHurstGHE(ts25[0], q=1, method='L2') *2)\n",
    "    H25[2].append(HSolver.EstHurstGHE(ts25[0], q=3, method='L2') *2)\n",
    "    H25[3].append(HSolver.EstHurstHiguchi(ts25[0],7, method='L2') *2)\n",
    "    # H25[3].append(1)\n",
    "    H25[4].append(mf_dfa(ts25[0]) *2)\n",
    "    H25[5].append(whittle(ts25[0])  *2)\n",
    "    H25[6].append(HSolver.EstHurstTTA(ts25[0],7, method='L2')  *2)\n",
    "    H25[7].append(HSolver.EstHurstPeriodogram(ts25[0], cutoff=0.5, method='L2')  *2)\n",
    "    H25[8].append(HSolver.EstHurstRSAnalysis2(ts25[0],2, method='L2')  *2)\n",
    "    H25[9].append(calculate_Hw(ts25[0])  *2)\n",
    "    H25[10].append(HSolver.EstHurstLocalWhittle(ts25[0])  *2)\n",
    "    H25[11].append(HSolver.EstHurstLSSD(ts25[0], max_scale) *2)\n",
    "    H25[12].append(HSolver.EstHurstLSV(ts25[0], max_scale)  *2)\n",
    "    H25[13].append(HSolver.EstHurstGHE(ts25[0], method='L2')  *2)\n",
    "        \n",
    "    H65[0].append(HSolver.EstHurstKS(ts65[0])  *2)\n",
    "    H65[1].append(HSolver.EstHurstGHE(ts65[0], q=1, method='L2') *2)\n",
    "    H65[2].append(HSolver.EstHurstGHE(ts65[0],q=3, method='L2')  *2)\n",
    "    H65[3].append(HSolver.EstHurstHiguchi(ts65[0], method='L2')  *2)\n",
    "    H65[4].append(mf_dfa(ts65[0]) *2)\n",
    "    H65[5].append(whittle(ts65[0])  *2)\n",
    "    H65[6].append(HSolver.EstHurstTTA(ts65[0],7, method='L2')  *2)\n",
    "    H65[7].append(HSolver.EstHurstPeriodogram(ts65[0], cutoff=0.3, method='L2')  *2)\n",
    "    H65[8].append(HSolver.EstHurstRSAnalysis2(ts65[0],4, method='L2')  *2)\n",
    "    H65[9].append(calculate_Hw(ts65[0])  *2)\n",
    "    H65[10].append(HSolver.EstHurstLocalWhittle(ts65[0])  *2)\n",
    "    H65[11].append(HSolver.EstHurstLSSD(ts65[0], max_scale)  *2)\n",
    "    H65[12].append(HSolver.EstHurstLSV(ts65[0], max_scale)  *2)\n",
    "    H65[13].append(HSolver.EstHurstGHE(ts65[0], method='L2')  *2)\n",
    "\n",
    "\n",
    "    H125[0].append(HSolver.EstHurstKS(ts125[0])  *2)\n",
    "    H125[1].append(HSolver.EstHurstGHE(ts125[0], q=1, method='L2') *2)\n",
    "    H125[2].append(HSolver.EstHurstGHE(ts125[0],q=3, method='L2')  *2)\n",
    "    H125[3].append(HSolver.EstHurstHiguchi(ts125[0], method='L2')  *2)\n",
    "    H125[4].append(mf_dfa(ts125[0]) *2)\n",
    "    H125[5].append(whittle(ts125[0])  *2)\n",
    "    H125[6].append(HSolver.EstHurstTTA(ts125[0],7, method='L2')  *2)\n",
    "    H125[7].append(HSolver.EstHurstPeriodogram(ts125[0], cutoff=0.3, method='L2')  *2)\n",
    "    H125[8].append(HSolver.EstHurstRSAnalysis2(ts125[0],4, method='L2')  *2)\n",
    "    H125[9].append(calculate_Hw(ts125[0])  *2)\n",
    "    H125[10].append(HSolver.EstHurstLocalWhittle(ts125[0])  *2)\n",
    "    H125[11].append(HSolver.EstHurstLSSD(ts125[0], max_scale)  *2)\n",
    "    H125[12].append(HSolver.EstHurstLSV(ts125[0], max_scale)  *2)\n",
    "    H125[13].append(HSolver.EstHurstGHE(ts125[0], method='L2') *2)\n",
    "\n",
    "    H225[0].append(HSolver.EstHurstKS(ts225[0])  *2)\n",
    "    H225[1].append(HSolver.EstHurstGHE(ts225[0], q=1, method='L2') *2)\n",
    "    H225[2].append(HSolver.EstHurstGHE(ts225[0],q=3, method='L2')  *2)\n",
    "    H225[3].append(HSolver.EstHurstHiguchi(ts225[0], method='L2')  *2)\n",
    "    H225[4].append(mf_dfa(ts225[0]) *2)\n",
    "    H225[5].append(whittle(ts225[0])  *2)\n",
    "    H225[6].append(HSolver.EstHurstTTA(ts225[0],11, method='L2')  *2)\n",
    "    H225[7].append(HSolver.EstHurstPeriodogram(ts225[0], cutoff=0.3, method='L2')  *2)\n",
    "    H225[8].append(HSolver.EstHurstRSAnalysis2(ts225[0],4, method='L2')  *2)\n",
    "    H225[9].append(calculate_Hw(ts225[0])  *2)\n",
    "    H225[10].append(HSolver.EstHurstLocalWhittle(ts225[0])  *2)\n",
    "    H225[11].append(HSolver.EstHurstLSSD(ts225[0], max_scale)  *2)\n",
    "    H225[12].append(HSolver.EstHurstLSV(ts225[0], max_scale)  *2)\n",
    "    H225[13].append(HSolver.EstHurstGHE(ts225[0], method='L2')  *2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f509e1",
   "metadata": {
    "papermill": {
     "duration": 0.023205,
     "end_time": "2025-12-29T18:43:28.204475",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.181270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparison of MAE for various Hurst exponent estimation methods on AnDi-2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce240cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:43:28.252682Z",
     "iopub.status.busy": "2025-12-29T18:43:28.252282Z",
     "iopub.status.idle": "2025-12-29T18:43:28.515447Z",
     "shell.execute_reply": "2025-12-29T18:43:28.514277Z"
    },
    "papermill": {
     "duration": 0.290945,
     "end_time": "2025-12-29T18:43:28.518911",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.227966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>KS</th>\n",
       "      <th>GHE(1)</th>\n",
       "      <th>GHE(3)</th>\n",
       "      <th>HM</th>\n",
       "      <th>MFDFA(2)</th>\n",
       "      <th>Whittle</th>\n",
       "      <th>TTA</th>\n",
       "      <th>PM</th>\n",
       "      <th>RS</th>\n",
       "      <th>db4</th>\n",
       "      <th>LW</th>\n",
       "      <th>LSSD</th>\n",
       "      <th>LSV</th>\n",
       "      <th>GHE(2)</th>\n",
       "      <th>TA-MSD</th>\n",
       "      <th>TF HurstEE</th>\n",
       "      <th>PyHurstEE</th>\n",
       "      <th>RANDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.467229</td>\n",
       "      <td>0.447670</td>\n",
       "      <td>0.426895</td>\n",
       "      <td>0.456333</td>\n",
       "      <td>0.451378</td>\n",
       "      <td>0.460595</td>\n",
       "      <td>0.737673</td>\n",
       "      <td>0.582725</td>\n",
       "      <td>0.838690</td>\n",
       "      <td>0.627909</td>\n",
       "      <td>0.520587</td>\n",
       "      <td>0.608502</td>\n",
       "      <td>0.538109</td>\n",
       "      <td>0.425563</td>\n",
       "      <td>0.425556</td>\n",
       "      <td>0.425628</td>\n",
       "      <td>0.425628</td>\n",
       "      <td>0.368934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.397337</td>\n",
       "      <td>0.392655</td>\n",
       "      <td>0.390099</td>\n",
       "      <td>0.412807</td>\n",
       "      <td>0.416830</td>\n",
       "      <td>0.583289</td>\n",
       "      <td>0.409720</td>\n",
       "      <td>0.504051</td>\n",
       "      <td>0.483933</td>\n",
       "      <td>0.447537</td>\n",
       "      <td>0.491047</td>\n",
       "      <td>0.450761</td>\n",
       "      <td>0.387381</td>\n",
       "      <td>0.387379</td>\n",
       "      <td>0.387509</td>\n",
       "      <td>0.387509</td>\n",
       "      <td>0.279289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>0.355204</td>\n",
       "      <td>0.370636</td>\n",
       "      <td>0.376730</td>\n",
       "      <td>0.358461</td>\n",
       "      <td>0.399246</td>\n",
       "      <td>0.396524</td>\n",
       "      <td>0.529739</td>\n",
       "      <td>0.387313</td>\n",
       "      <td>0.436964</td>\n",
       "      <td>0.433633</td>\n",
       "      <td>0.414907</td>\n",
       "      <td>0.459110</td>\n",
       "      <td>0.423265</td>\n",
       "      <td>0.370725</td>\n",
       "      <td>0.370725</td>\n",
       "      <td>0.370926</td>\n",
       "      <td>0.370926</td>\n",
       "      <td>0.251573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225</td>\n",
       "      <td>0.338480</td>\n",
       "      <td>0.363480</td>\n",
       "      <td>0.373087</td>\n",
       "      <td>0.339248</td>\n",
       "      <td>0.403521</td>\n",
       "      <td>0.388477</td>\n",
       "      <td>0.456913</td>\n",
       "      <td>0.378311</td>\n",
       "      <td>0.409407</td>\n",
       "      <td>0.415201</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.442715</td>\n",
       "      <td>0.410889</td>\n",
       "      <td>0.368438</td>\n",
       "      <td>0.368437</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.255844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n",
       "0      25  0.467229  0.447670  0.426895  0.456333  0.451378  0.460595   \n",
       "1      65  0.375900  0.397337  0.392655  0.390099  0.412807  0.416830   \n",
       "2     125  0.355204  0.370636  0.376730  0.358461  0.399246  0.396524   \n",
       "3     225  0.338480  0.363480  0.373087  0.339248  0.403521  0.388477   \n",
       "\n",
       "        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n",
       "0  0.737673  0.582725  0.838690  0.627909  0.520587  0.608502  0.538109   \n",
       "1  0.583289  0.409720  0.504051  0.483933  0.447537  0.491047  0.450761   \n",
       "2  0.529739  0.387313  0.436964  0.433633  0.414907  0.459110  0.423265   \n",
       "3  0.456913  0.378311  0.409407  0.415201  0.399132  0.442715  0.410889   \n",
       "\n",
       "     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n",
       "0  0.425563  0.425556    0.425628   0.425628  0.368934  \n",
       "1  0.387381  0.387379    0.387509   0.387509  0.279289  \n",
       "2  0.370725  0.370725    0.370926   0.370926  0.251573  \n",
       "3  0.368438  0.368437    0.368433   0.368433  0.255844  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE H_noised\n",
    "dic = {}\n",
    "dic['length'] = [25, 65, 125, 225]\n",
    "for i in range(14):\n",
    "    dic[methods[i]] = [mean_absolute_error(np.nan_to_num(H25[i], nan=1.0), Y25[0]), mean_absolute_error(np.nan_to_num(H65[i], nan=1.0), Y65[0]),  mean_absolute_error(np.nan_to_num(H125[i], nan=1.0), Y125[0]), mean_absolute_error(np.nan_to_num(H225[i], nan=1.0), Y225[0])]\n",
    "dic[methods[14]] = [mae_tamsd25, mae_tamsd65, mae_tamsd125, mae_tamsd225]\n",
    "dic[methods[15]] = [mae_tfmsd25, mae_tfmsd65, mae_tfmsd125, mae_tfmsd225]\n",
    "dic[methods[16]] = [mae_pymsd25, mae_pymsd65, mae_pymsd125, mae_pymsd225]\n",
    "dic[methods[17]] = [mae_randi_25, mae_randi_65, mae_randi_125, mae_randi_225]\n",
    "andi1_df = pd.DataFrame(dic)\n",
    "andi1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7dc77",
   "metadata": {
    "papermill": {
     "duration": 0.031823,
     "end_time": "2025-12-29T18:43:28.582407",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.550584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare FBM Dataset based on AnDi-2024 Single-state model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b040c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:43:28.639147Z",
     "iopub.status.busy": "2025-12-29T18:43:28.638775Z",
     "iopub.status.idle": "2025-12-29T18:43:28.646694Z",
     "shell.execute_reply": "2025-12-29T18:43:28.645634Z"
    },
    "papermill": {
     "duration": 0.035192,
     "end_time": "2025-12-29T18:43:28.648475",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.613283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  from https://github.com/AnDiChallenge/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stochastic.processes.noise import FractionalGaussianNoise as FGN\n",
    "import warnings\n",
    "import scipy.stats\n",
    "def gaussian(params:list|int, # If list, mu and sigma^2 of the gaussian. If int, we consider sigma = 0\n",
    "             size = 1,  # Number of samples to get.\n",
    "             bound = None # Bound of the Gaussian, if any.\n",
    "            )-> np.array: # Samples from the given Gaussian distribution\n",
    "    '''\n",
    "    Samples from a Gaussian distribution of given parameters.\n",
    "    '''\n",
    "    # if we are given a single number, we consider equal to mean and variance = 0\n",
    "    if isinstance(params, float) or isinstance(params, int):\n",
    "        if size == 1:\n",
    "            return params\n",
    "        else:\n",
    "            return np.array(params).repeat(size)\n",
    "    else:\n",
    "        mean, var = params        \n",
    "        if bound is None:\n",
    "            val = np.random.normal(mean, np.sqrt(var), size)\n",
    "        if bound is not None:\n",
    "            lower, upper = bound\n",
    "            if var == 0: \n",
    "                if mean > upper or mean < lower:\n",
    "                    raise ValueError('Demanded value outside of range.')\n",
    "                val = np.ones(size)*mean\n",
    "            else:\n",
    "                val = scipy.stats.truncnorm.rvs((lower-mean)/np.sqrt(var),\n",
    "                                                (upper-mean)/np.sqrt(var),\n",
    "                                                loc = mean,\n",
    "                                                scale = np.sqrt(var),\n",
    "                                                size = size)\n",
    "        if size == 1:\n",
    "            return val[0]\n",
    "        else:\n",
    "            return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cefa36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:43:28.698516Z",
     "iopub.status.busy": "2025-12-29T18:43:28.698126Z",
     "iopub.status.idle": "2025-12-29T18:43:28.732329Z",
     "shell.execute_reply": "2025-12-29T18:43:28.731466Z"
    },
    "papermill": {
     "duration": 0.061126,
     "end_time": "2025-12-29T18:43:28.734085",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.672959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset generration from https://github.com/AnDiChallenge/\n",
    "\n",
    "# %% ../source_nbs/lib_nbs/models_phenom.ipynb 5\n",
    "class models_phenom():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This class handles the generation of trajectories from different theoretical models. \n",
    "        ''' \n",
    "        # We define here the bounds of the anomalous exponent and diffusion coefficient\n",
    "        self.bound_D = [1e-12, 1e6]\n",
    "        self.bound_alpha = [0, 1.999]\n",
    "        \n",
    "        # We also define the value in which we consider directed motion\n",
    "        self.alpha_directed = 1.9\n",
    "        \n",
    "        # Diffusion state labels: the position of each type defines its numerical label\n",
    "        # i: immobile/trapped; c: confined; f: free-diffusive (normal and anomalous); d: directed\n",
    "        self.lab_state = ['i', 'c', 'f', 'd']\n",
    "\n",
    "# %% ../source_nbs/lib_nbs/models_phenom.ipynb 7\n",
    "class models_phenom(models_phenom):\n",
    "    \n",
    "    @staticmethod\n",
    "    def disp_fbm(alpha : float,\n",
    "                 D : float,\n",
    "                 T: int, \n",
    "                 deltaT : int = 1):\n",
    "        ''' Generates normalized Fractional Gaussian noise. This means that, in \n",
    "        general:\n",
    "        $$\n",
    "        <x^2(t) > = 2Dt^{alpha}\n",
    "        $$\n",
    "                            \n",
    "        and in particular:\n",
    "        $$\n",
    "        <x^2(t = 1)> = 2D \n",
    "        $$\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float in [0,2]\n",
    "            Anomalous exponent\n",
    "        D : float\n",
    "            Diffusion coefficient\n",
    "        T : int\n",
    "            Number of displacements to generate\n",
    "        deltaT : int, optional\n",
    "            Sampling time\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Array containing T displacements of given parameters\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Generate displacements\n",
    "        disp = FGN(hurst = alpha/2).sample(n = T)\n",
    "        # Normalization factor\n",
    "        disp *= np.sqrt(T)**(alpha)\n",
    "        # Add D\n",
    "        disp *= np.sqrt(2*D*deltaT)        \n",
    "        \n",
    "        return disp\n",
    "\n",
    "# %% ../source_nbs/lib_nbs/models_phenom.ipynb 18\n",
    "class models_phenom(models_phenom):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _constraint_alpha(alpha_1, alpha_2, epsilon_a):\n",
    "        ''' Defines the metric for constraining the changes in anomalous\n",
    "        exponent'''\n",
    "        return alpha_1 - alpha_2 < epsilon_a\n",
    "    \n",
    "    @staticmethod\n",
    "    def _constraint_d(d1, d2, gamma_d):\n",
    "        ''' Defines the metric for constraining the changes in anomalous\n",
    "        exponent'''\n",
    "        if gamma_d < 1:\n",
    "            return d2 > d1*gamma_d\n",
    "        if gamma_d > 1:\n",
    "            return d2 < d1*gamma_d\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sample_diff_parameters(alphas : list, # List containing the parameters to sample anomalous exponent in state (adapt to sampling function)\n",
    "                                Ds : list, # List containing the parameters to sample the diffusion coefficient in state (adapt to sampling function).\n",
    "                                num_states : int, # Number of diffusive states.\n",
    "                                epsilon_a : float, #  Minimum distance between anomalous exponents of various states.\n",
    "                                gamma_d : float, # Factor between diffusion coefficient of various states.\n",
    "                               ) : \n",
    "        '''    \n",
    "        Given information of the anomalous exponents (alphas), diffusion coefficients (Ds), the function\n",
    "        samples these from a bounded Gaussian distribution with the indicated constraints (epsilon_a,\n",
    "        gamma_d). Outputs the list of demanded alphas and Ds.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alphas : list\n",
    "        List containing the parameters to sample anomalous exponent in state (adapt to sampling function).\n",
    "        Ds : list\n",
    "        List containing the parameters to sample the diffusion coefficient in state (adapt to sampling function).\n",
    "        num_states : int\n",
    "        Number of diffusive states.\n",
    "        epsilon_a : float\n",
    "        Minimum distance between anomalous exponents of various states.            \n",
    "                epsilon workflow: we check val[i] - val[i-1] < epsilon\n",
    "                    if you want that val[i] > val[i-1]: epsilon has to be positive\n",
    "                    if you want that val[i] < val[i-1]: epsilon has to be negative\n",
    "                    if you don't care: epsilon = 0\n",
    "                \n",
    "        gamma_d : float\n",
    "        Factor between diffusion coefficient of various states.            \n",
    "                gamma workflow: \n",
    "                    for gamma < 1: val[i] < val[i-1]*gamma\n",
    "                    for gamma > 1: val[i] > val[i-1]*gamma\n",
    "                    for gamma = 1: no check\n",
    "        Returns\n",
    "        -------\n",
    "            :alphas_traj (list): list of anomalous exponents\n",
    "            :Ds_traj (list): list of diffusion coefficients\n",
    "                      \n",
    "        '''\n",
    "\n",
    "        \n",
    "        alphas_traj = []\n",
    "        Ds_traj = []\n",
    "        for i in range(num_states): \n",
    "\n",
    "            # for the first state we just sample normally\n",
    "            if i == 0:\n",
    "                alphas_traj.append(float(gaussian(alphas[i], bound = models_phenom().bound_alpha)))\n",
    "                Ds_traj.append(float(gaussian(Ds[i], bound = models_phenom().bound_D)))\n",
    "           \n",
    "            # For next states we take into account epsilon distance between diffusion\n",
    "            # parameter\n",
    "            else:\n",
    "                ## Checking alpha\n",
    "                alpha_state = float(gaussian(alphas[i], bound = models_phenom().bound_alpha))\n",
    "                D_state = float(gaussian(Ds[i], bound = models_phenom().bound_D))\n",
    "\n",
    "                if epsilon_a[i-1] != 0:\n",
    "                    idx_while = 0\n",
    "                    while models_phenom()._constraint_alpha(alphas_traj[-1], alpha_state, epsilon_a[i-1]):\n",
    "                    #alphas_traj[-1] - alpha_state < epsilon_a[i-1]:\n",
    "                        alpha_state = float(gaussian(alphas[i], bound = models_phenom().bound_alpha))                        \n",
    "                        idx_while += 1\n",
    "                        if idx_while > 100: # check that we are not stuck forever in the while loop\n",
    "                            raise FileNotFoundError(f'Could not find correct alpha for state {i} in 100 steps. State distributions probably too close.')\n",
    "\n",
    "                alphas_traj.append(alpha_state)\n",
    "                \n",
    "                ## Checking D\n",
    "                if gamma_d[i-1] != 1:    \n",
    "                    \n",
    "                    idx_while = 0\n",
    "                    while models_phenom()._constraint_d(Ds_traj[-1], D_state, gamma_d[i-1]):\n",
    "                        D_state = float(gaussian(Ds[i], bound = models_phenom().bound_D))\n",
    "                        idx_while += 1\n",
    "                        if idx_while > 100: # check that we are not stuck forever in the while loop\n",
    "                            raise FileNotFoundError(f'Could not find correct D for state {i} in 100 steps. State distributions probably too close.')\n",
    "               \n",
    "    \n",
    "                Ds_traj.append(D_state)\n",
    "                \n",
    "        return alphas_traj, Ds_traj\n",
    "\n",
    "# %% ../source_nbs/lib_nbs/models_phenom.ipynb 26\n",
    "class models_phenom(models_phenom):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _single_state_traj(T :int = 200, \n",
    "                          D : float = 1, \n",
    "                          alpha : float = 1, \n",
    "                          L : float = None,\n",
    "                          deltaT : int = 1,\n",
    "                          dim : int = 2):\n",
    "        '''\n",
    "        Generates a single state trajectory with given parameters. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Length of the trajectory\n",
    "        D : float\n",
    "            Diffusion coefficient       \n",
    "        alpha : float\n",
    "            Anomalous exponent\n",
    "        L : float\n",
    "            Length of the box acting as the environment\n",
    "        deltaT : int, optional\n",
    "            Sampling time\n",
    "        dim : int\n",
    "            Dimension of the walk (can be 2 or 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            - pos: position of the particle\n",
    "            - labels:  anomalous exponent, D and state at each timestep. State is always free here.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # Trajectory displacements\n",
    "        disp_d = []\n",
    "        for d in range(dim):\n",
    "            disp_d.append(models_phenom().disp_fbm(alpha, D, T))\n",
    "        # Labels\n",
    "        lab_diff_state = np.ones(T)*models_phenom().lab_state.index('f') if alpha < models_phenom().alpha_directed else np.ones(T)*models_phenom().lab_state.index('d')\n",
    "        labels = np.vstack((np.ones(T)*alpha, \n",
    "                            np.ones(T)*D,\n",
    "                            lab_diff_state\n",
    "                           )).transpose()\n",
    "\n",
    "        # If there are no boundaries\n",
    "        if not L:\n",
    "            \n",
    "            pos = np.vstack([np.cumsum(disp)-disp[0] for disp in disp_d]).transpose()\n",
    "            \n",
    "            return pos, labels\n",
    "\n",
    "        # If there are, apply reflecting boundary conditions\n",
    "        else:\n",
    "            pos = np.zeros((T, dim))\n",
    "\n",
    "            # Initialize the particle in a random position of the box\n",
    "            pos[0, :] = np.random.rand(dim)*L\n",
    "            for t in range(1, T):\n",
    "                if dim == 2:\n",
    "                    pos[t, :] = [pos[t-1, 0]+disp_d[0][t], \n",
    "                                 pos[t-1, 1]+disp_d[1][t]]            \n",
    "                elif dim == 3:\n",
    "                    pos[t, :] = [pos[t-1, 0]+disp_d[0][t], \n",
    "                                 pos[t-1, 1]+disp_d[1][t], \n",
    "                                 pos[t-1, 2]+disp_d[2][t]]            \n",
    "\n",
    "\n",
    "                # Reflecting boundary conditions\n",
    "                while np.max(pos[t, :])>L or np.min(pos[t, :])< 0: \n",
    "                    pos[t, pos[t, :] > L] = pos[t, pos[t, :] > L] - 2*(pos[t, pos[t, :] > L] - L)\n",
    "                    pos[t, pos[t, :] < 0] = - pos[t, pos[t, :] < 0]\n",
    "\n",
    "            return pos, labels\n",
    "\n",
    "# %% ../source_nbs/lib_nbs/models_phenom.ipynb 33\n",
    "class models_phenom(models_phenom):\n",
    "    \n",
    "    \n",
    "    def single_state(self,\n",
    "                     N:int = 10,\n",
    "                     T:int = 200, \n",
    "                     Ds:list = [1, 0], \n",
    "                     alphas:list = [1, 0], \n",
    "                     L:float = None,\n",
    "                     dim:int = 2):\n",
    "        '''\n",
    "        Generates a dataset made of single state trajectories with given parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int, list\n",
    "            Number of trajectories in the dataset\n",
    "        T : int\n",
    "            Length of the trajectory\n",
    "        Ds : float\n",
    "            If list, mean and variance from which to sample the diffusion coefficient. If float, we consider variance = 0.\n",
    "        alphas : float\n",
    "            If list, mean and variance from which to sample the anomalous exponent. If float, we consider variance = 0.\n",
    "        L : float\n",
    "            Length of the box acting as the environment\n",
    "        deltaT : int, optional\n",
    "            Sampling time            \n",
    "        dim : int\n",
    "            Dimension of the walk (can be 2 or 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            - positions: position of the N trajectories.\n",
    "            - labels:  anomalous exponent, D and state at each timestep. State is always free here.         \n",
    "        '''\n",
    "\n",
    "        positions = np.zeros((T, N, dim))\n",
    "        labels = np.zeros((T, N, 3))\n",
    "\n",
    "        for n in range(N):\n",
    "            alpha_traj = gaussian(alphas, bound = self.bound_alpha)\n",
    "            D_traj = gaussian(Ds, bound = self.bound_D)\n",
    "            # Get trajectory from single traj function\n",
    "            pos, lab = self._single_state_traj(T = T, \n",
    "                                               D = D_traj, \n",
    "                                               alpha = alpha_traj, \n",
    "                                               L = L,\n",
    "                                               dim = dim\n",
    "                                              )        \n",
    "            positions[:, n, :] = pos\n",
    "            labels[:, n, :] = lab\n",
    "\n",
    "        return positions, labels\n",
    "\n",
    "models_class = models_phenom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c6de26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:43:28.782992Z",
     "iopub.status.busy": "2025-12-29T18:43:28.782568Z",
     "iopub.status.idle": "2025-12-29T18:43:46.852847Z",
     "shell.execute_reply": "2025-12-29T18:43:46.851550Z"
    },
    "papermill": {
     "duration": 18.096539,
     "end_time": "2025-12-29T18:43:46.854567",
     "exception": false,
     "start_time": "2025-12-29T18:43:28.758028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 8000, 2)\n"
     ]
    }
   ],
   "source": [
    "traj_length = 25\n",
    "\n",
    "X25, Y25 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n",
    "\n",
    "traj_length = 65\n",
    "\n",
    "X65, Y65 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n",
    "\n",
    "traj_length = 125\n",
    "\n",
    "X125, Y125 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n",
    "\n",
    "traj_length = 225\n",
    "\n",
    "X225, Y225 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n",
    "\n",
    "print(X225.shape)\n",
    "\n",
    "Y25, Y65, Y125, Y225 = [Y25[0,:,0]], [Y65[0,:,0]], [Y125[0,:,0]], [Y225[0,:,0]]\n",
    "X25 = [X25[:,:,:1].transpose(1, 2, 0)]\n",
    "X65 = [X65[:,:,:1].transpose(1, 2, 0)]\n",
    "X125 = [X125[:,:,:1].transpose(1, 2, 0)]\n",
    "X225 = [X225[:,:,:1].transpose(1, 2, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0280b",
   "metadata": {
    "papermill": {
     "duration": 0.023319,
     "end_time": "2025-12-29T18:43:46.902179",
     "exception": false,
     "start_time": "2025-12-29T18:43:46.878860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RANDI prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64ebb275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:43:46.950929Z",
     "iopub.status.busy": "2025-12-29T18:43:46.950514Z",
     "iopub.status.idle": "2025-12-29T18:44:15.368850Z",
     "shell.execute_reply": "2025-12-29T18:44:15.367842Z"
    },
    "papermill": {
     "duration": 28.445084,
     "end_time": "2025-12-29T18:44:15.370769",
     "exception": false,
     "start_time": "2025-12-29T18:43:46.925685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bs25 = randi25.layers[0].input_shape[-1]\n",
    "bs65 = randi65.layers[0].input_shape[-1]\n",
    "bs125 = randi125.layers[0].input_shape[-1]\n",
    "bs225 = randi225.layers[0].input_shape[-1]\n",
    "#normalizing the data\n",
    "data25 = data_norm(X25[0],dim=1,task=1)\n",
    "data65 = data_norm(X65[0],dim=1,task=1)\n",
    "data125 = data_norm(X125[0],dim=1,task=1)\n",
    "data225 = data_norm(X225[0],dim=1,task=1)\n",
    "#reshaping the data\n",
    "data_rs25 = data_reshape(data25,bs=bs25,dim=1)\n",
    "data_rs65 = data_reshape(data65,bs=bs65,dim=1)\n",
    "data_rs125 = data_reshape(data125,bs=bs125,dim=1)\n",
    "data_rs225 = data_reshape(data225,bs=bs225,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length \n",
    "pred_25 = randi25.predict(data_rs25)\n",
    "pred_65 = randi65.predict(data_rs65)\n",
    "pred_125 = randi65.predict(data_rs125)\n",
    "pred_225 = randi65.predict(data_rs225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bee58c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:15.421130Z",
     "iopub.status.busy": "2025-12-29T18:44:15.420770Z",
     "iopub.status.idle": "2025-12-29T18:44:15.431054Z",
     "shell.execute_reply": "2025-12-29T18:44:15.429845Z"
    },
    "papermill": {
     "duration": 0.03736,
     "end_time": "2025-12-29T18:44:15.432850",
     "exception": false,
     "start_time": "2025-12-29T18:44:15.395490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDI25 MAE : 0.3158377239231142\n",
      "RANDI65 MAE : 0.231976482794604\n",
      "RANDI125 MAE : 0.2132926452925448\n",
      "RANDI225 MAE : 0.21981351410683206\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE with scikit-learn\n",
    "mae_randi_25 = mean_absolute_error(pred_25.flatten(), Y25[0])\n",
    "mae_randi_65 = mean_absolute_error(pred_65.flatten(), Y65[0])\n",
    "mae_randi_125 = mean_absolute_error(pred_125.flatten(), Y125[0])\n",
    "mae_randi_225 = mean_absolute_error(pred_225.flatten(), Y225[0])\n",
    "print(\"RANDI25 MAE :\", mae_randi_25)\n",
    "print(\"RANDI65 MAE :\", mae_randi_65)\n",
    "print(\"RANDI125 MAE :\", mae_randi_125)\n",
    "print(\"RANDI225 MAE :\", mae_randi_225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa9050",
   "metadata": {
    "papermill": {
     "duration": 0.023386,
     "end_time": "2025-12-29T18:44:15.480238",
     "exception": false,
     "start_time": "2025-12-29T18:44:15.456852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TA-MSD prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a3d5127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:15.529318Z",
     "iopub.status.busy": "2025-12-29T18:44:15.528896Z",
     "iopub.status.idle": "2025-12-29T18:44:15.545456Z",
     "shell.execute_reply": "2025-12-29T18:44:15.543913Z"
    },
    "papermill": {
     "duration": 0.043127,
     "end_time": "2025-12-29T18:44:15.547320",
     "exception": false,
     "start_time": "2025-12-29T18:44:15.504193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 25, 1) (8000, 65, 1) (8000, 125, 1) (8000, 225, 1)\n"
     ]
    }
   ],
   "source": [
    "cumdata25 = np.array(X25[0]).reshape(-1,1,25).transpose(0,2,1)\n",
    "cumdata65 = np.array(X65[0]).reshape(-1,1,65).transpose(0,2,1)\n",
    "cumdata125 = np.array(X125[0]).reshape(-1,1,125).transpose(0,2,1)\n",
    "cumdata225 = np.array(X225[0]).reshape(-1,1,225).transpose(0,2,1)\n",
    "print(cumdata25.shape, cumdata65.shape, cumdata125.shape, cumdata225.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "572b3647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:15.596657Z",
     "iopub.status.busy": "2025-12-29T18:44:15.596267Z",
     "iopub.status.idle": "2025-12-29T18:44:18.559974Z",
     "shell.execute_reply": "2025-12-29T18:44:18.558880Z"
    },
    "papermill": {
     "duration": 2.993536,
     "end_time": "2025-12-29T18:44:18.565018",
     "exception": false,
     "start_time": "2025-12-29T18:44:15.571482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tamsd25 = Tamsd()(cumdata25) \n",
    "tamsd65 = Tamsd()(cumdata65) \n",
    "tamsd125 = Tamsd()(cumdata125) \n",
    "tamsd225 = Tamsd()(cumdata225) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18260786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:18.625424Z",
     "iopub.status.busy": "2025-12-29T18:44:18.625065Z",
     "iopub.status.idle": "2025-12-29T18:44:18.634514Z",
     "shell.execute_reply": "2025-12-29T18:44:18.633490Z"
    },
    "papermill": {
     "duration": 0.038351,
     "end_time": "2025-12-29T18:44:18.636418",
     "exception": false,
     "start_time": "2025-12-29T18:44:18.598067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSD MAE 25 : 0.21377105752030995\n",
      "MSD MAE 65 : 0.12764270439226588\n",
      "MSD MAE 125: 0.09400767418443319\n",
      "MSD MAE 225: 0.07068072092003108\n"
     ]
    }
   ],
   "source": [
    "mae_tamsd25 = mean_absolute_error(tamsd25, Y25[0])\n",
    "mae_tamsd65 = mean_absolute_error(tamsd65, Y65[0])\n",
    "mae_tamsd125 = mean_absolute_error(tamsd125, Y125[0])\n",
    "mae_tamsd225 = mean_absolute_error(tamsd225, Y225[0])\n",
    "print(\"MSD MAE 25 :\", mae_tamsd25)\n",
    "print(\"MSD MAE 65 :\", mae_tamsd65)\n",
    "print(\"MSD MAE 125:\", mae_tamsd125)\n",
    "print(\"MSD MAE 225:\", mae_tamsd225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6b4b6",
   "metadata": {
    "papermill": {
     "duration": 0.023957,
     "end_time": "2025-12-29T18:44:18.684923",
     "exception": false,
     "start_time": "2025-12-29T18:44:18.660966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HurstEE Layer prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c7e0ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:18.734574Z",
     "iopub.status.busy": "2025-12-29T18:44:18.734185Z",
     "iopub.status.idle": "2025-12-29T18:44:19.001230Z",
     "shell.execute_reply": "2025-12-29T18:44:18.999938Z"
    },
    "papermill": {
     "duration": 0.294144,
     "end_time": "2025-12-29T18:44:19.003127",
     "exception": false,
     "start_time": "2025-12-29T18:44:18.708983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_MSD MAE 25 : 0.21377105603741905\n",
      "TF_MSD MAE 65 : 0.1276427038939353\n",
      "TF_MSD MAE 125: 0.09400767859206115\n",
      "TF_MSD MAE 225: 0.07068072183871726\n"
     ]
    }
   ],
   "source": [
    "tf_msd = HurstEE()\n",
    "tfmsd25 = tf_msd(cumdata25).numpy() * 2\n",
    "tfmsd65 = tf_msd(cumdata65).numpy() * 2\n",
    "tfmsd125 = tf_msd(cumdata125).numpy() * 2\n",
    "tfmsd225 = tf_msd(cumdata225).numpy() * 2\n",
    "mae_tfmsd25 = mean_absolute_error(np.nan_to_num(tfmsd25, nan=1.0), Y25[0])\n",
    "mae_tfmsd65 = mean_absolute_error(np.nan_to_num(tfmsd65, nan=1.0), Y65[0])\n",
    "mae_tfmsd125 = mean_absolute_error(np.nan_to_num(tfmsd125, nan=1.0), Y125[0])\n",
    "mae_tfmsd225 = mean_absolute_error(np.nan_to_num(tfmsd225, nan=1.0), Y225[0])\n",
    "\n",
    "\n",
    "print(\"TF_MSD MAE 25 :\", mae_tfmsd25)\n",
    "print(\"TF_MSD MAE 65 :\", mae_tfmsd65)\n",
    "print(\"TF_MSD MAE 125:\", mae_tfmsd125)\n",
    "print(\"TF_MSD MAE 225:\", mae_tfmsd225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46ce4ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:19.053862Z",
     "iopub.status.busy": "2025-12-29T18:44:19.053498Z",
     "iopub.status.idle": "2025-12-29T18:44:19.179729Z",
     "shell.execute_reply": "2025-12-29T18:44:19.178516Z"
    },
    "papermill": {
     "duration": 0.15341,
     "end_time": "2025-12-29T18:44:19.181535",
     "exception": false,
     "start_time": "2025-12-29T18:44:19.028125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyHurstEELayer MAE 25 : 0.2137710580298391\n",
      "PyHurstEELayer MAE 65 : 0.12764269895647046\n",
      "PyHurstEELayer MAE 125: 0.09400768101016746\n",
      "PyHurstEELayer MAE 225: 0.07068072183767862\n"
     ]
    }
   ],
   "source": [
    "pyhurstLayer = PyHurstEE()\n",
    "pymsd25 = pyhurstLayer(cumdata25).detach().cpu().numpy() * 2\n",
    "pymsd65 = pyhurstLayer(cumdata65).detach().cpu().numpy() * 2\n",
    "pymsd125 = pyhurstLayer(cumdata125).detach().cpu().numpy() * 2\n",
    "pymsd225 = pyhurstLayer(cumdata225).detach().cpu().numpy() * 2\n",
    "mae_pymsd25 = mean_absolute_error(np.nan_to_num(pymsd25, nan=1.0), Y25[0])\n",
    "mae_pymsd65 = mean_absolute_error(np.nan_to_num(pymsd65, nan=1.0), Y65[0])\n",
    "mae_pymsd125 = mean_absolute_error(np.nan_to_num(pymsd125, nan=1.0), Y125[0])\n",
    "mae_pymsd225 = mean_absolute_error(np.nan_to_num(pymsd225, nan=1.0), Y225[0])\n",
    "\n",
    "\n",
    "print(\"PyHurstEELayer MAE 25 :\", mae_pymsd25)\n",
    "print(\"PyHurstEELayer MAE 65 :\", mae_pymsd65)\n",
    "print(\"PyHurstEELayer MAE 125:\", mae_pymsd125)\n",
    "print(\"PyHurstEELayer MAE 225:\", mae_pymsd225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df440af",
   "metadata": {
    "papermill": {
     "duration": 0.027449,
     "end_time": "2025-12-29T18:44:19.233820",
     "exception": false,
     "start_time": "2025-12-29T18:44:19.206371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Estimate Hurst exponent with statistical methods (FBM dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "701da1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:44:19.283923Z",
     "iopub.status.busy": "2025-12-29T18:44:19.283571Z",
     "iopub.status.idle": "2025-12-29T19:17:26.062198Z",
     "shell.execute_reply": "2025-12-29T19:17:26.061365Z"
    },
    "papermill": {
     "duration": 1986.805924,
     "end_time": "2025-12-29T19:17:26.064151",
     "exception": false,
     "start_time": "2025-12-29T18:44:19.258227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "H25 = [[] for _ in range(len(methods))]\n",
    "H65 = [[] for _ in range(len(methods))]\n",
    "H125 = [[] for _ in range(len(methods))]\n",
    "H225 = [[] for _ in range(len(methods))]\n",
    "\n",
    "minimal = 3\n",
    "max_scale = 11\n",
    "\n",
    "for row in range(len(X25[0])):\n",
    "    x25 = np.array(X25[0][row]).reshape(1,-1)\n",
    "    ts25=x25[:,1:]-x25[:,:-1]\n",
    "    ts25 = ts25 / np.std(ts25)\n",
    "    x65 = np.array(X65[0][row]).reshape(1,-1)\n",
    "    ts65=x65[:,1:]-x65[:,:-1]\n",
    "    ts65 = ts65 / np.std(ts65)\n",
    "    x125 = np.array(X125[0][row]).reshape(1,-1)\n",
    "    ts125 = x125[:,1:]-x125[:,:-1]\n",
    "    ts125 = ts125 / np.std(ts125)\n",
    "    x225 = np.array(X225[0][row]).reshape(1,-1)\n",
    "    ts225 = x225[:,1:]-x225[:,:-1]\n",
    "    ts225 = ts225 / np.std(ts225)\n",
    "    \n",
    "    H25[0].append(HSolver.EstHurstKS(ts25[0]) *2)\n",
    "    H25[1].append(HSolver.EstHurstGHE(ts25[0], q=1, method='L2') *2)\n",
    "    H25[2].append(HSolver.EstHurstGHE(ts25[0], q=3, method='L2') *2)\n",
    "    H25[3].append(HSolver.EstHurstHiguchi(ts25[0],7, method='L2') *2)\n",
    "    # H25[3].append(1)\n",
    "    H25[4].append(mf_dfa(ts25[0]) *2)\n",
    "    H25[5].append(whittle(ts25[0])  *2)\n",
    "    H25[6].append(HSolver.EstHurstTTA(ts25[0],7, method='L2')  *2)\n",
    "    H25[7].append(HSolver.EstHurstPeriodogram(ts25[0], cutoff=0.5, method='L2')  *2)\n",
    "    H25[8].append(HSolver.EstHurstRSAnalysis2(ts25[0],2, method='L2')  *2)\n",
    "    H25[9].append(calculate_Hw(ts25[0])  *2)\n",
    "    H25[10].append(HSolver.EstHurstLocalWhittle(ts25[0])  *2)\n",
    "    H25[11].append(HSolver.EstHurstLSSD(ts25[0], max_scale) *2)\n",
    "    H25[12].append(HSolver.EstHurstLSV(ts25[0], max_scale)  *2)\n",
    "    H25[13].append(HSolver.EstHurstGHE(ts25[0], method='L2')  *2)\n",
    "        \n",
    "    H65[0].append(HSolver.EstHurstKS(ts65[0])  *2)\n",
    "    H65[1].append(HSolver.EstHurstGHE(ts65[0], q=1, method='L2') *2)\n",
    "    H65[2].append(HSolver.EstHurstGHE(ts65[0],q=3, method='L2')  *2)\n",
    "    H65[3].append(HSolver.EstHurstHiguchi(ts65[0], method='L2')  *2)\n",
    "    H65[4].append(mf_dfa(ts65[0]) *2)\n",
    "    H65[5].append(whittle(ts65[0])  *2)\n",
    "    H65[6].append(HSolver.EstHurstTTA(ts65[0],7, method='L2')  *2)\n",
    "    H65[7].append(HSolver.EstHurstPeriodogram(ts65[0], cutoff=0.3, method='L2')  *2)\n",
    "    H65[8].append(HSolver.EstHurstRSAnalysis2(ts65[0],4, method='L2')  *2)\n",
    "    H65[9].append(calculate_Hw(ts65[0])  *2)\n",
    "    H65[10].append(HSolver.EstHurstLocalWhittle(ts65[0])  *2)\n",
    "    H65[11].append(HSolver.EstHurstLSSD(ts65[0], max_scale)  *2)\n",
    "    H65[12].append(HSolver.EstHurstLSV(ts65[0], max_scale)  *2)\n",
    "    H65[13].append(HSolver.EstHurstGHE(ts65[0], method='L2')  *2)\n",
    "\n",
    "\n",
    "    H125[0].append(HSolver.EstHurstKS(ts125[0])  *2)\n",
    "    H125[1].append(HSolver.EstHurstGHE(ts125[0], q=1, method='L2') *2)\n",
    "    H125[2].append(HSolver.EstHurstGHE(ts125[0],q=3, method='L2')  *2)\n",
    "    H125[3].append(HSolver.EstHurstHiguchi(ts125[0], method='L2')  *2)\n",
    "    H125[4].append(mf_dfa(ts125[0]) *2)\n",
    "    H125[5].append(whittle(ts125[0])  *2)\n",
    "    H125[6].append(HSolver.EstHurstTTA(ts125[0],7, method='L2')  *2)\n",
    "    H125[7].append(HSolver.EstHurstPeriodogram(ts125[0], cutoff=0.3, method='L2')  *2)\n",
    "    H125[8].append(HSolver.EstHurstRSAnalysis2(ts125[0],4, method='L2')  *2)\n",
    "    H125[9].append(calculate_Hw(ts125[0])  *2)\n",
    "    H125[10].append(HSolver.EstHurstLocalWhittle(ts125[0])  *2)\n",
    "    H125[11].append(HSolver.EstHurstLSSD(ts125[0], max_scale)  *2)\n",
    "    H125[12].append(HSolver.EstHurstLSV(ts125[0], max_scale)  *2)\n",
    "    H125[13].append(HSolver.EstHurstGHE(ts125[0], method='L2') *2)\n",
    "\n",
    "    H225[0].append(HSolver.EstHurstKS(ts225[0])  *2)\n",
    "    H225[1].append(HSolver.EstHurstGHE(ts225[0], q=1, method='L2') *2)\n",
    "    H225[2].append(HSolver.EstHurstGHE(ts225[0],q=3, method='L2')  *2)\n",
    "    H225[3].append(HSolver.EstHurstHiguchi(ts225[0], method='L2')  *2)\n",
    "    H225[4].append(mf_dfa(ts225[0]) *2)\n",
    "    H225[5].append(whittle(ts225[0])  *2)\n",
    "    H225[6].append(HSolver.EstHurstTTA(ts225[0],11, method='L2')  *2)\n",
    "    H225[7].append(HSolver.EstHurstPeriodogram(ts225[0], cutoff=0.3, method='L2')  *2)\n",
    "    H225[8].append(HSolver.EstHurstRSAnalysis2(ts225[0],4, method='L2')  *2)\n",
    "    H225[9].append(calculate_Hw(ts225[0])  *2)\n",
    "    H225[10].append(HSolver.EstHurstLocalWhittle(ts225[0])  *2)\n",
    "    H225[11].append(HSolver.EstHurstLSSD(ts225[0], max_scale)  *2)\n",
    "    H225[12].append(HSolver.EstHurstLSV(ts225[0], max_scale)  *2)\n",
    "    H225[13].append(HSolver.EstHurstGHE(ts225[0], method='L2')  *2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f3f9f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T19:17:26.114287Z",
     "iopub.status.busy": "2025-12-29T19:17:26.113942Z",
     "iopub.status.idle": "2025-12-29T19:17:26.226791Z",
     "shell.execute_reply": "2025-12-29T19:17:26.225935Z"
    },
    "papermill": {
     "duration": 0.139489,
     "end_time": "2025-12-29T19:17:26.228746",
     "exception": false,
     "start_time": "2025-12-29T19:17:26.089257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic['length'] = [25, 65, 125, 225]\n",
    "for i in range(14):\n",
    "    dic[methods[i]] = [mean_absolute_error(np.nan_to_num(H25[i], nan=1.0), Y25[0]), mean_absolute_error(np.nan_to_num(H65[i], nan=1.0), Y65[0]),  mean_absolute_error(np.nan_to_num(H125[i], nan=1.0), Y125[0]), mean_absolute_error(np.nan_to_num(H225[i], nan=1.0), Y225[0])]\n",
    "dic[methods[14]] = [mae_tamsd25, mae_tamsd65, mae_tamsd125, mae_tamsd225]\n",
    "dic[methods[15]] = [mae_tfmsd25, mae_tfmsd65, mae_tfmsd125, mae_tfmsd225]\n",
    "dic[methods[16]] = [mae_pymsd25, mae_pymsd65, mae_pymsd125, mae_pymsd225]\n",
    "dic[methods[17]] = [mae_randi_25, mae_randi_65, mae_randi_125, mae_randi_225]\n",
    "fbm_df = pd.DataFrame(dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e7733",
   "metadata": {
    "papermill": {
     "duration": 0.023312,
     "end_time": "2025-12-29T19:17:26.276824",
     "exception": false,
     "start_time": "2025-12-29T19:17:26.253512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparison of MAE for AnDi-2020 and FBM Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a814a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T19:17:26.325444Z",
     "iopub.status.busy": "2025-12-29T19:17:26.325083Z",
     "iopub.status.idle": "2025-12-29T19:17:26.344595Z",
     "shell.execute_reply": "2025-12-29T19:17:26.343698Z"
    },
    "papermill": {
     "duration": 0.045583,
     "end_time": "2025-12-29T19:17:26.346137",
     "exception": false,
     "start_time": "2025-12-29T19:17:26.300554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnDi-1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>KS</th>\n",
       "      <th>GHE(1)</th>\n",
       "      <th>GHE(3)</th>\n",
       "      <th>HM</th>\n",
       "      <th>MFDFA(2)</th>\n",
       "      <th>Whittle</th>\n",
       "      <th>TTA</th>\n",
       "      <th>PM</th>\n",
       "      <th>RS</th>\n",
       "      <th>db4</th>\n",
       "      <th>LW</th>\n",
       "      <th>LSSD</th>\n",
       "      <th>LSV</th>\n",
       "      <th>GHE(2)</th>\n",
       "      <th>TA-MSD</th>\n",
       "      <th>TF HurstEE</th>\n",
       "      <th>PyHurstEE</th>\n",
       "      <th>RANDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.467229</td>\n",
       "      <td>0.447670</td>\n",
       "      <td>0.426895</td>\n",
       "      <td>0.456333</td>\n",
       "      <td>0.451378</td>\n",
       "      <td>0.460595</td>\n",
       "      <td>0.737673</td>\n",
       "      <td>0.582725</td>\n",
       "      <td>0.838690</td>\n",
       "      <td>0.627909</td>\n",
       "      <td>0.520587</td>\n",
       "      <td>0.608502</td>\n",
       "      <td>0.538109</td>\n",
       "      <td>0.425563</td>\n",
       "      <td>0.425556</td>\n",
       "      <td>0.425628</td>\n",
       "      <td>0.425628</td>\n",
       "      <td>0.368934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.397337</td>\n",
       "      <td>0.392655</td>\n",
       "      <td>0.390099</td>\n",
       "      <td>0.412807</td>\n",
       "      <td>0.416830</td>\n",
       "      <td>0.583289</td>\n",
       "      <td>0.409720</td>\n",
       "      <td>0.504051</td>\n",
       "      <td>0.483933</td>\n",
       "      <td>0.447537</td>\n",
       "      <td>0.491047</td>\n",
       "      <td>0.450761</td>\n",
       "      <td>0.387381</td>\n",
       "      <td>0.387379</td>\n",
       "      <td>0.387509</td>\n",
       "      <td>0.387509</td>\n",
       "      <td>0.279289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>0.355204</td>\n",
       "      <td>0.370636</td>\n",
       "      <td>0.376730</td>\n",
       "      <td>0.358461</td>\n",
       "      <td>0.399246</td>\n",
       "      <td>0.396524</td>\n",
       "      <td>0.529739</td>\n",
       "      <td>0.387313</td>\n",
       "      <td>0.436964</td>\n",
       "      <td>0.433633</td>\n",
       "      <td>0.414907</td>\n",
       "      <td>0.459110</td>\n",
       "      <td>0.423265</td>\n",
       "      <td>0.370725</td>\n",
       "      <td>0.370725</td>\n",
       "      <td>0.370926</td>\n",
       "      <td>0.370926</td>\n",
       "      <td>0.251573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225</td>\n",
       "      <td>0.338480</td>\n",
       "      <td>0.363480</td>\n",
       "      <td>0.373087</td>\n",
       "      <td>0.339248</td>\n",
       "      <td>0.403521</td>\n",
       "      <td>0.388477</td>\n",
       "      <td>0.456913</td>\n",
       "      <td>0.378311</td>\n",
       "      <td>0.409407</td>\n",
       "      <td>0.415201</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.442715</td>\n",
       "      <td>0.410889</td>\n",
       "      <td>0.368438</td>\n",
       "      <td>0.368437</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.368433</td>\n",
       "      <td>0.255844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n",
       "0      25  0.467229  0.447670  0.426895  0.456333  0.451378  0.460595   \n",
       "1      65  0.375900  0.397337  0.392655  0.390099  0.412807  0.416830   \n",
       "2     125  0.355204  0.370636  0.376730  0.358461  0.399246  0.396524   \n",
       "3     225  0.338480  0.363480  0.373087  0.339248  0.403521  0.388477   \n",
       "\n",
       "        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n",
       "0  0.737673  0.582725  0.838690  0.627909  0.520587  0.608502  0.538109   \n",
       "1  0.583289  0.409720  0.504051  0.483933  0.447537  0.491047  0.450761   \n",
       "2  0.529739  0.387313  0.436964  0.433633  0.414907  0.459110  0.423265   \n",
       "3  0.456913  0.378311  0.409407  0.415201  0.399132  0.442715  0.410889   \n",
       "\n",
       "     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n",
       "0  0.425563  0.425556    0.425628   0.425628  0.368934  \n",
       "1  0.387381  0.387379    0.387509   0.387509  0.279289  \n",
       "2  0.370725  0.370725    0.370926   0.370926  0.251573  \n",
       "3  0.368438  0.368437    0.368433   0.368433  0.255844  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('AnDi-1:')\n",
    "andi1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a25829a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T19:17:26.395729Z",
     "iopub.status.busy": "2025-12-29T19:17:26.395311Z",
     "iopub.status.idle": "2025-12-29T19:17:26.414234Z",
     "shell.execute_reply": "2025-12-29T19:17:26.413264Z"
    },
    "papermill": {
     "duration": 0.045332,
     "end_time": "2025-12-29T19:17:26.415843",
     "exception": false,
     "start_time": "2025-12-29T19:17:26.370511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>KS</th>\n",
       "      <th>GHE(1)</th>\n",
       "      <th>GHE(3)</th>\n",
       "      <th>HM</th>\n",
       "      <th>MFDFA(2)</th>\n",
       "      <th>Whittle</th>\n",
       "      <th>TTA</th>\n",
       "      <th>PM</th>\n",
       "      <th>RS</th>\n",
       "      <th>db4</th>\n",
       "      <th>LW</th>\n",
       "      <th>LSSD</th>\n",
       "      <th>LSV</th>\n",
       "      <th>GHE(2)</th>\n",
       "      <th>TA-MSD</th>\n",
       "      <th>TF HurstEE</th>\n",
       "      <th>PyHurstEE</th>\n",
       "      <th>RANDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.313945</td>\n",
       "      <td>0.230594</td>\n",
       "      <td>0.218156</td>\n",
       "      <td>0.250719</td>\n",
       "      <td>0.303877</td>\n",
       "      <td>0.241937</td>\n",
       "      <td>0.521188</td>\n",
       "      <td>0.509782</td>\n",
       "      <td>0.717681</td>\n",
       "      <td>0.481472</td>\n",
       "      <td>0.294398</td>\n",
       "      <td>0.406343</td>\n",
       "      <td>0.316171</td>\n",
       "      <td>0.213780</td>\n",
       "      <td>0.213771</td>\n",
       "      <td>0.213771</td>\n",
       "      <td>0.213771</td>\n",
       "      <td>0.315838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>0.151546</td>\n",
       "      <td>0.137575</td>\n",
       "      <td>0.132297</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.234533</td>\n",
       "      <td>0.132747</td>\n",
       "      <td>0.288387</td>\n",
       "      <td>0.235090</td>\n",
       "      <td>0.352028</td>\n",
       "      <td>0.295095</td>\n",
       "      <td>0.168467</td>\n",
       "      <td>0.226181</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.127646</td>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.231976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>0.113385</td>\n",
       "      <td>0.102802</td>\n",
       "      <td>0.096969</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.214879</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>0.204501</td>\n",
       "      <td>0.216857</td>\n",
       "      <td>0.280962</td>\n",
       "      <td>0.201341</td>\n",
       "      <td>0.127896</td>\n",
       "      <td>0.156519</td>\n",
       "      <td>0.125569</td>\n",
       "      <td>0.094009</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.213293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.076793</td>\n",
       "      <td>0.073782</td>\n",
       "      <td>0.086923</td>\n",
       "      <td>0.207572</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>0.133945</td>\n",
       "      <td>0.213482</td>\n",
       "      <td>0.241612</td>\n",
       "      <td>0.202303</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.113894</td>\n",
       "      <td>0.094356</td>\n",
       "      <td>0.070682</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>0.219814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n",
       "0      25  0.313945  0.230594  0.218156  0.250719  0.303877  0.241937   \n",
       "1      65  0.151546  0.137575  0.132297  0.169330  0.234533  0.132747   \n",
       "2     125  0.113385  0.102802  0.096969  0.121120  0.214879  0.092318   \n",
       "3     225  0.084928  0.076793  0.073782  0.086923  0.207572  0.067514   \n",
       "\n",
       "        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n",
       "0  0.521188  0.509782  0.717681  0.481472  0.294398  0.406343  0.316171   \n",
       "1  0.288387  0.235090  0.352028  0.295095  0.168467  0.226181  0.176364   \n",
       "2  0.204501  0.216857  0.280962  0.201341  0.127896  0.156519  0.125569   \n",
       "3  0.133945  0.213482  0.241612  0.202303  0.104379  0.113894  0.094356   \n",
       "\n",
       "     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n",
       "0  0.213780  0.213771    0.213771   0.213771  0.315838  \n",
       "1  0.127646  0.127643    0.127643   0.127643  0.231976  \n",
       "2  0.094009  0.094008    0.094008   0.094008  0.213293  \n",
       "3  0.070682  0.070681    0.070681   0.070681  0.219814  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('FBM:')\n",
    "fbm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c47d7",
   "metadata": {
    "papermill": {
     "duration": 0.024018,
     "end_time": "2025-12-29T19:17:26.464738",
     "exception": false,
     "start_time": "2025-12-29T19:17:26.440720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7098592,
     "sourceId": 11345187,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4233.368446,
   "end_time": "2025-12-29T19:17:29.501960",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-29T18:06:56.133514",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
